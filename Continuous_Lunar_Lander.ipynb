{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "je2oRXsPFSmI"
      },
      "outputs": [],
      "source": [
        "!wget http://www.atarimania.com/roms/Roms.rar\n",
        "!unrar x -o+ /content/Roms.rar >/dev/nul\n",
        "!python -m atari_py.import_roms /content/ROMS >/dev/nul\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "!apt-get update > /dev/null 2>&1\n",
        "!apt-get install cmake > /dev/null 2>&1\n",
        "!pip install --upgrade setuptools 2>&1\n",
        "!pip install ez_setup > /dev/null 2>&1\n",
        "!pip install gym[atari] > /dev/null 2>&1\n",
        "!pip install swig\n",
        "!pip install box2d\n",
        "!pip install gymnasium\n",
        "!pip3 install box2d box2d-kengz"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import os, sys\n",
        "import argparse\n",
        "from gymnasium.spaces import Discrete, Box, Tuple, MultiDiscrete, Dict\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import gc\n",
        "import time\n",
        "import scipy as sp\n",
        "import pprint\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Normal\n",
        "import torch.optim as optim\n",
        "import torch.nn.init as init\n",
        "import copy\n",
        "import random\n",
        "from collections import namedtuple, deque"
      ],
      "metadata": {
        "id": "_jvGSRDtF-yV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayBuffer:\n",
        "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
        "\n",
        "    def __init__(self, buffer_size, batch_size, seed=12345):\n",
        "        \"\"\"Initialize a ReplayBuffer object.\n",
        "        Params\n",
        "        ======\n",
        "            buffer_size (int): maximum size of buffer\n",
        "            batch_size (int): size of each training batch\n",
        "        \"\"\"\n",
        "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        self.memory = deque(maxlen=buffer_size)  # internal memory (deque)\n",
        "        self.batch_size = batch_size\n",
        "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
        "        self.seed = random.seed(seed)\n",
        "\n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Add a new experience to memory.\"\"\"\n",
        "        e = self.experience(state, action, reward, next_state, done)\n",
        "        self.memory.append(e)\n",
        "\n",
        "    def sample(self):\n",
        "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
        "        if len(self.memory) <= self.batch_size:\n",
        "            return None\n",
        "\n",
        "        experiences = random.sample(self.memory, k=self.batch_size)\n",
        "\n",
        "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(self.device)\n",
        "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(self.device)\n",
        "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(self.device)\n",
        "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(self.device)\n",
        "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(self.device)\n",
        "\n",
        "        return (states, actions, rewards, next_states, dones)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Return the current size of internal memory.\"\"\"\n",
        "        return len(self.memory)"
      ],
      "metadata": {
        "id": "CgXr3XWHFzx-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class OUNoise:\n",
        "    \"\"\"Ornstein-Uhlenbeck process.\"\"\"\n",
        "\n",
        "    def __init__(self, size, seed=12345, mu=0., theta=0.15, sigma=0.2):\n",
        "        \"\"\"Initialize parameters and noise process.\"\"\"\n",
        "        self.size = size\n",
        "        self.mu = mu * np.ones(size)\n",
        "        self.theta = theta\n",
        "        self.sigma = sigma\n",
        "        self.seed = random.seed(seed)\n",
        "\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Reset the internal state (= noise) to mean (mu).\"\"\"\n",
        "        self.state = copy.copy(self.mu)\n",
        "\n",
        "    def sample(self):\n",
        "        \"\"\"Update internal state and return it as a noise sample.\"\"\"\n",
        "        x = self.state\n",
        "        dx = self.theta * (self.mu - x) + self.sigma * np.random.standard_normal(self.size)\n",
        "        self.state = x + dx\n",
        "\n",
        "        return self.state\n",
        "\n",
        "class GaussianNoise:\n",
        "    \"\"\"Gaussian noise.\"\"\"\n",
        "\n",
        "    def __init__(self, size, seed, mu=0, sigma=1):\n",
        "        \"\"\"Initialize parameters and noise process.\"\"\"\n",
        "        self.size = size\n",
        "        self.mu = mu\n",
        "        self.sigma = sigma\n",
        "        self.seed = random.seed(seed)\n",
        "\n",
        "    def reset(self):\n",
        "        pass\n",
        "\n",
        "    def sample(self):\n",
        "        \"\"\"Return Gaussian perturbations in the action space.\"\"\"\n",
        "        noise = np.random.normal(0, self.sigma, self.size)\n",
        "        return noise"
      ],
      "metadata": {
        "id": "BGk6Tz4bGsso"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ActorNetwork(nn.Module):\n",
        "    def __init__(self, alpha, input_dims, fc1_dims, fc2_dims,\n",
        "                 n_actions, weight_decay, noise, add_noise=True, name='ActorNetwork', chkpt_dir='/Checkpoints', seed=12345):\n",
        "        super(ActorNetwork, self).__init__()\n",
        "\n",
        "        self.chkpt_file = os.path.join(chkpt_dir, name)\n",
        "\n",
        "        layers = [\n",
        "            nn.Linear(input_dims, fc1_dims),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(fc1_dims, fc2_dims),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(fc2_dims, n_actions),\n",
        "            nn.Tanh()\n",
        "        ]\n",
        "\n",
        "        self.fc_layers = nn.Sequential(*layers)\n",
        "        self.learning_rate = alpha\n",
        "        self.weight_decay = weight_decay\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=alpha, weight_decay=self.weight_decay)\n",
        "        self.noise = noise\n",
        "        self.add_noise = add_noise\n",
        "\n",
        "        self.action_space = None\n",
        "\n",
        "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        self.init_weights()\n",
        "        self.to(self.device)\n",
        "\n",
        "    def set_action_space(self, action_space):\n",
        "        self.action_space = action_space\n",
        "\n",
        "    def init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                init.xavier_uniform_(m.weight)\n",
        "                init.constant_(m.bias, 0.0)\n",
        "\n",
        "    def forward(self, state):\n",
        "        pi = self.fc_layers(state)\n",
        "        pi_scaled = pi * torch.from_numpy(self.action_space.high).float()\n",
        "\n",
        "        return pi_scaled\n",
        "\n",
        "    def select_action(self, state, add_noise=True):\n",
        "        with torch.no_grad():\n",
        "            state = torch.tensor(state).to(self.device)\n",
        "            state = state.unsqueeze(0)\n",
        "            action = self.forward(state)\n",
        "            action = action.detach().cpu().numpy()[0]\n",
        "\n",
        "        if add_noise:\n",
        "            action += self.noise.sample()\n",
        "\n",
        "        action = action.clip(min=self.action_space.low, max=self.action_space.high)\n",
        "\n",
        "        return action\n",
        "\n",
        "    def backpropagation(self, state, critic_model):\n",
        "        actions_pred = self.fc_layers(state)\n",
        "        actor_loss = -critic_model.forward(state, actions_pred).mean()\n",
        "        self.optimizer.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "\n",
        "    def save_checkpoint(self):\n",
        "        torch.save(self.state_dict(), self.chkpt_file)\n",
        "\n",
        "    def load_checkpoint(self):\n",
        "        self.load_state_dict(torch.load(self.chkpt_file))\n",
        "\n",
        "\n",
        "class CriticNetwork(nn.Module):\n",
        "    def __init__(self, beta, input_dims, fc1_dims, fc2_dims,\n",
        "                n_actions, weight_decay, name='CriticNetwork', chkpt_dir='/Checkpoints'):\n",
        "        super(CriticNetwork, self).__init__()\n",
        "\n",
        "        self.chkpt_file = os.path.join(chkpt_dir, name)\n",
        "        layers = [\n",
        "            nn.Linear(input_dims + n_actions, fc1_dims),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(fc1_dims, fc2_dims),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(fc2_dims, 1)\n",
        "        ]\n",
        "\n",
        "        self.fc_layers = nn.Sequential(*layers)\n",
        "        self.learning_rate = beta\n",
        "\n",
        "        self.weight_decay = weight_decay\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=beta, weight_decay=self.weight_decay)\n",
        "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        self.to(self.device)\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                init.xavier_uniform_(m.weight)\n",
        "                init.constant_(m.bias, 0.0)\n",
        "\n",
        "    def forward(self, state, action):\n",
        "        sa = torch.cat([state, action], dim=1)\n",
        "        q_value = self.fc_layers(sa)\n",
        "\n",
        "        return q_value\n",
        "\n",
        "    def backpropagation(self, qvals_target, expected_qvals):\n",
        "\n",
        "        critic_loss = F.mse_loss(expected_qvals, qvals_target)\n",
        "        self.optimizer.zero_grad()\n",
        "        critic_loss.backward()\n",
        "\n",
        "        # Clip gradients:\n",
        "        for param in self.parameters():\n",
        "            param.grad.data.clamp_(-10, 10)\n",
        "\n",
        "        self.optimizer.step()\n",
        "\n",
        "    def save_checkpoint(self):\n",
        "        torch.save(self.state_dict(), self.chkpt_file)\n",
        "\n",
        "    def load_checkpoint(self):\n",
        "        self.load_state_dict(torch.load(self.chkpt_file))"
      ],
      "metadata": {
        "id": "TuLO9JvkG3vO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent:\n",
        "    def __init__(self, actor_dims, critic_dims, n_actions, actor_output_dims,\n",
        "                    alpha=0.01, beta=0.01, fc1=64,\n",
        "                    fc2=64, gamma=0.95, tau=0.01,\n",
        "                    add_noise=True,\n",
        "                    weight_decay=0.001, memory_size=100000, memory_batch_size=64,\n",
        "                    chkpt_dir='tmp/maddpg/'):\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta\n",
        "        self.gamma = gamma\n",
        "        self.tau = tau\n",
        "        self.n_actions = n_actions\n",
        "        self.actor_output_dims = actor_output_dims\n",
        "        self.add_noise = add_noise\n",
        "        self.noise = OUNoise(size=self.actor_output_dims)\n",
        "        self.weight_decay = weight_decay\n",
        "        self.memory_size = memory_size\n",
        "        self.memory_batch_size = memory_batch_size\n",
        "        self.experience_replay = ReplayBuffer(buffer_size=memory_size, batch_size=self.memory_batch_size)\n",
        "        self.agent_name = 'LunarLander'\n",
        "        self.actor = ActorNetwork(alpha=self.alpha, input_dims=actor_dims,\n",
        "                                  fc1_dims=fc1, fc2_dims=fc2,\n",
        "                                  n_actions=self.n_actions, weight_decay=self.weight_decay,\n",
        "                                  noise=self.noise, add_noise=self.add_noise,\n",
        "                                  chkpt_dir=chkpt_dir,\n",
        "                                  name=self.agent_name+'_actor.pth')\n",
        "        self.critic = CriticNetwork(beta=self.beta, input_dims=critic_dims, fc1_dims=fc1, fc2_dims=fc2,\n",
        "                                    n_actions=self.n_actions, weight_decay=self.weight_decay,\n",
        "                                    chkpt_dir=chkpt_dir,\n",
        "                                    name=self.agent_name+'_critic.pth')\n",
        "        self.target_actor = ActorNetwork(alpha=self.alpha, input_dims=actor_dims,\n",
        "                                        fc1_dims=fc1, fc2_dims=fc2,\n",
        "                                        n_actions=self.n_actions, weight_decay=self.weight_decay,\n",
        "                                        noise=self.noise, add_noise=self.add_noise,\n",
        "                                        chkpt_dir=chkpt_dir,\n",
        "                                        name=self.agent_name+'_target_actor.pth')\n",
        "        self.target_critic = CriticNetwork(beta=self.beta, input_dims=critic_dims, fc1_dims=fc1, fc2_dims=fc2,\n",
        "                                            n_actions=self.n_actions, weight_decay=self.weight_decay,\n",
        "                                            chkpt_dir=chkpt_dir,\n",
        "                                            name=self.agent_name+'_target_critic.pth')\n",
        "\n",
        "        self.update_network_parameters(tau=self.tau)\n",
        "\n",
        "\n",
        "    def update_network_parameters(self, tau=None):\n",
        "        if tau is None:\n",
        "            tau = self.tau\n",
        "\n",
        "        for param, target_param in zip(self.critic.parameters(), self.target_critic.parameters()):\n",
        "            target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "\n",
        "        for param, target_param in zip(self.actor.parameters(), self.target_actor.parameters()):\n",
        "            target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "\n",
        "        # target_actor_params = self.target_actor.named_parameters()\n",
        "        # actor_params = self.actor.named_parameters()\n",
        "\n",
        "        # target_actor_state_dict = dict(target_actor_params)\n",
        "        # actor_state_dict = dict(actor_params)\n",
        "        # for name in actor_state_dict:\n",
        "        #     actor_state_dict[name] = tau*actor_state_dict[name].clone() + \\\n",
        "        #             (1-tau)*target_actor_state_dict[name].clone()\n",
        "\n",
        "        # self.target_actor.load_state_dict(actor_state_dict)\n",
        "\n",
        "        # target_critic_params = self.target_critic.named_parameters()\n",
        "        # critic_params = self.critic.named_parameters()\n",
        "\n",
        "        # target_critic_state_dict = dict(target_critic_params)\n",
        "        # critic_state_dict = dict(critic_params)\n",
        "        # for name in critic_state_dict:\n",
        "        #     critic_state_dict[name] = tau*critic_state_dict[name].clone() + \\\n",
        "        #             (1-tau)*target_critic_state_dict[name].clone()\n",
        "\n",
        "        # self.target_critic.load_state_dict(critic_state_dict)\n",
        "\n",
        "    def sample_to_memory(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Save experience in replay memory, and use random sample from buffer to learn.\"\"\"\n",
        "        # Save experience / reward\n",
        "        self.experience_replay.add(state, action, reward, next_state, done)\n",
        "\n",
        "    def sample_from_memory(self):\n",
        "        # Learn, if enough samples are available in memory\n",
        "        minibatch_experiences = self.experience_replay.sample()\n",
        "        if not minibatch_experiences:\n",
        "            return\n",
        "\n",
        "        return minibatch_experiences\n",
        "\n",
        "    def choose_action(self, state, add_noise=True):\n",
        "        action = self.actor.select_action(state, add_noise)\n",
        "        return action\n",
        "\n",
        "    def step(self):\n",
        "\n",
        "        minibatch_experiences = self.sample_from_memory()\n",
        "        states, actions, rewards, next_states, dones = minibatch_experiences\n",
        "        next_actions = self.target_actor.forward(next_states)\n",
        "        Q_targets_next = self.target_critic.forward(next_states, next_actions)\n",
        "        Q_targets = rewards + self.gamma * Q_targets_next * (1 - dones)\n",
        "        Q_expected = self.critic.forward(states, actions)\n",
        "        self.critic.backpropagation(Q_targets, Q_expected)\n",
        "\n",
        "        pred_actions = self.actor.forward(states)\n",
        "        actor_loss = - self.critic(states, pred_actions).mean()\n",
        "        # Minimize the loss\n",
        "        self.actor.optimizer.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        self.actor.optimizer.step()\n",
        "\n",
        "        self.update_network_parameters(tau=self.tau)\n",
        "\n",
        "    def save_models(self):\n",
        "        self.actor.save_checkpoint()\n",
        "        self.target_actor.save_checkpoint()\n",
        "        self.critic.save_checkpoint()\n",
        "        self.target_critic.save_checkpoint()\n",
        "\n",
        "    def load_models(self):\n",
        "        self.actor.load_checkpoint()\n",
        "        self.target_actor.load_checkpoint()\n",
        "        self.critic.load_checkpoint()\n",
        "        self.target_critic.load_checkpoint()\n"
      ],
      "metadata": {
        "id": "j3Xy73txH0po"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lunar Lander"
      ],
      "metadata": {
        "id": "3_uUQrQZIFj-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\n",
        "    \"LunarLander-v2\",\n",
        "    continuous=True,\n",
        "    gravity=-9.81,\n",
        "    enable_wind=False,\n",
        "    wind_power=5.0,\n",
        "    turbulence_power=1.5,\n",
        ")"
      ],
      "metadata": {
        "id": "QSHrbcsSH6Dv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "action_space = env.action_space\n",
        "observation_space = env.observation_space\n",
        "\n",
        "print(\"Action Space:\", action_space.shape[0])\n",
        "print(\"Observation Space:\", observation_space.shape[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNR7aF_dI47P",
        "outputId": "d07cc1b9-159b-45f6-8b4c-33ca59827deb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Action Space: 2\n",
            "Observation Space: 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LunarLander:\n",
        "    def __init__(self,\n",
        "                 n_episodes=1000, memory_size=1e6, memory_batch_size=64,\n",
        "                 alpha=0.01, beta=0.01, fc1=64,\n",
        "                 fc2=64, gamma=0.99, tau=0.01, chkpt_dir='tmp/maddpg/'):\n",
        "\n",
        "        self.env = env = gym.make(\n",
        "                            \"LunarLander-v2\",\n",
        "                            continuous=True,\n",
        "                            gravity=-9.81,\n",
        "                            enable_wind=False,\n",
        "                            wind_power=5.0,\n",
        "                            turbulence_power=1.5,\n",
        "                        )\n",
        "\n",
        "        self.n_episodes = n_episodes\n",
        "\n",
        "        self.n_actions = self.env.action_space.shape[0]\n",
        "        self.actor_output_dims = self.env.action_space.shape\n",
        "        self.actor_dims = self.env.observation_space.shape[0]\n",
        "        self.crtic_dims = self.env.observation_space.shape[0]\n",
        "\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta\n",
        "        self.fc1 = fc1\n",
        "        self.fc2 = fc2\n",
        "        self.gamma = gamma\n",
        "        self.tau = tau\n",
        "        self.memory_size = memory_size\n",
        "        self.batch_size = memory_batch_size\n",
        "        self.chkpt_dir = chkpt_dir\n",
        "\n",
        "        self.agent = Agent(actor_dims=self.actor_dims,\n",
        "                           critic_dims=self.crtic_dims,\n",
        "                           n_actions=self.n_actions,\n",
        "                           actor_output_dims=self.actor_output_dims,\n",
        "                           alpha=self.alpha,\n",
        "                           beta=self.beta,\n",
        "                           fc1=self.fc1, fc2=self.fc2, gamma=self.gamma, tau=self.tau,\n",
        "                           memory_size=self.memory_size, memory_batch_size=self.batch_size,\n",
        "                           chkpt_dir=self.chkpt_dir)\n",
        "\n",
        "        self.training_steps = 0\n",
        "\n",
        "        self.agent.target_actor.set_action_space(self.env.action_space)\n",
        "        self.agent.actor.set_action_space(self.env.action_space)\n",
        "\n",
        "        self.score_record = []\n",
        "\n",
        "    def env_reset(self):\n",
        "        state = self.env.reset()\n",
        "        if isinstance(state, tuple):\n",
        "            state = state[0]  # Index the first element of the tuple\n",
        "        state = torch.tensor(state, dtype=torch.float32)\n",
        "        return state\n",
        "\n",
        "\n",
        "    def save_checkpoint(self):\n",
        "        print('... saving checkpoint ...')\n",
        "        self.agent.save_models()\n",
        "\n",
        "    def load_checkpoint(self):\n",
        "        print('... loading checkpoint ...')\n",
        "        self.agent.load_models()\n",
        "\n",
        "\n",
        "    def train(self, load=False):\n",
        "        if load:\n",
        "            self.load_checkpoint()\n",
        "\n",
        "        score_history = []\n",
        "        for episode in range(self.n_episodes):\n",
        "\n",
        "            state = self.env_reset()\n",
        "\n",
        "            episode_score = 0\n",
        "            done = False\n",
        "\n",
        "            while not done:\n",
        "\n",
        "                if self.training_steps <= (1/10) * self.memory_size or len(self.agent.experience_replay) <= self.batch_size:\n",
        "                    action = self.env.action_space.sample()\n",
        "                    next_state, reward, done, _, _ = self.env.step(action_space.sample())\n",
        "                    self.agent.sample_to_memory(state, action, reward, next_state, done)\n",
        "                else:\n",
        "                    action = self.agent.choose_action(state, add_noise=True)\n",
        "                    next_state, reward, done, _, _ = self.env.step(action)\n",
        "                    self.agent.sample_to_memory(state, action, reward, next_state, done)\n",
        "                    state = next_state\n",
        "                    self.agent.step()\n",
        "\n",
        "                self.training_steps += 1\n",
        "                episode_score += reward\n",
        "\n",
        "            score_history.append(episode_score)\n",
        "            self.score_record.append(np.mean(score_history))\n",
        "\n",
        "            if episode % 10 == 0 and episode>20:\n",
        "                print('Episode: ', episode, 'Score: ', np.mean(score_history[-10:-1]))\n",
        "                self.save_checkpoint()\n",
        "                plt.clf()  # Clear the previous plot\n",
        "                plt.figure(figsize=(15,10))\n",
        "                plt.plot(score_history)\n",
        "                plt.xlabel('Episode')\n",
        "                plt.ylabel('Score')\n",
        "                plt.title('Score History')\n",
        "                mva = np.convolve(score_history, np.ones(10)/10, mode='valid')\n",
        "                x = np.arange(len(mva))\n",
        "                plt.plot(x, mva, label='Moving Average')\n",
        "\n",
        "                plt.legend()  # Show the legend with labels\n",
        "                plt.pause(0.001)  # Add a small pause to allow the plot to be displayed\n",
        "\n",
        "    def test(self):\n",
        "\n",
        "        self.load_checkpoint()\n",
        "\n",
        "        state = self.env_reset()\n",
        "\n",
        "        episode_score = 0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            action = self.agent.choose_action(state, add_noise=False)\n",
        "            next_state, reward, done, _, _ = self.env.step(action)\n",
        "            state = next_state\n",
        "            episode_score += reward\n",
        "\n",
        "        print(f'Episode Score: {episode_score}')\n",
        "\n"
      ],
      "metadata": {
        "id": "0s8N-sfMJ1ra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_env = LunarLander(n_episodes=300,\n",
        "                           memory_size=100000,\n",
        "                           memory_batch_size=256,\n",
        "                           alpha=1e-4, beta=1e-3,\n",
        "                           fc1=256,\n",
        "                           fc2=128,\n",
        "                           gamma=0.995,\n",
        "                           tau=1e-3,\n",
        "                           chkpt_dir='/content')"
      ],
      "metadata": {
        "id": "z3KmDwldPVDK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_env.train(load=True)"
      ],
      "metadata": {
        "id": "y-L2M9nYQHE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_env.test()"
      ],
      "metadata": {
        "id": "4SdUQy1tcAD8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}