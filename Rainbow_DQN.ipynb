{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": [
        "2qdO5vKpHNpb",
        "feyM3bt2HrGP",
        "JE7avOE-KCGr",
        "mfwdEHnXKfkZ"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import and Installing"
      ],
      "metadata": {
        "id": "2qdO5vKpHNpb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bOhh63eoHJHm",
        "outputId": "f87bd299-1ecf-40a1-8fb7-3cf732461a12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "IN_COLAB = \"google.colab\" in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    !apt install python-opengl\n",
        "    !apt install ffmpeg\n",
        "    !apt install xvfb\n",
        "    !pip install pyvirtualdisplay\n",
        "    !pip install -U gym\n",
        "    from pyvirtualdisplay import Display\n",
        "\n",
        "    # Start virtual display\n",
        "    dis = Display(visible=0, size=(400, 400))\n",
        "    dis.start()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x74jf9nUHUYP",
        "outputId": "6d3841f5-574d-4fce-b0d3-34db933596f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "python-opengl is already the newest version (3.1.0+dfsg-1).\n",
            "The following packages were automatically installed and are no longer required:\n",
            "  cuda-command-line-tools-10-0 cuda-command-line-tools-10-1\n",
            "  cuda-command-line-tools-11-0 cuda-compiler-10-0 cuda-compiler-10-1\n",
            "  cuda-compiler-11-0 cuda-cuobjdump-10-0 cuda-cuobjdump-10-1\n",
            "  cuda-cuobjdump-11-0 cuda-cupti-10-0 cuda-cupti-10-1 cuda-cupti-11-0\n",
            "  cuda-cupti-dev-11-0 cuda-documentation-10-0 cuda-documentation-10-1\n",
            "  cuda-documentation-11-0 cuda-documentation-11-1 cuda-gdb-10-0 cuda-gdb-10-1\n",
            "  cuda-gdb-11-0 cuda-gpu-library-advisor-10-0 cuda-gpu-library-advisor-10-1\n",
            "  cuda-libraries-10-0 cuda-libraries-10-1 cuda-libraries-11-0\n",
            "  cuda-memcheck-10-0 cuda-memcheck-10-1 cuda-memcheck-11-0 cuda-nsight-10-0\n",
            "  cuda-nsight-10-1 cuda-nsight-11-0 cuda-nsight-11-1 cuda-nsight-compute-10-0\n",
            "  cuda-nsight-compute-10-1 cuda-nsight-compute-11-0 cuda-nsight-compute-11-1\n",
            "  cuda-nsight-systems-10-1 cuda-nsight-systems-11-0 cuda-nsight-systems-11-1\n",
            "  cuda-nvcc-10-0 cuda-nvcc-10-1 cuda-nvcc-11-0 cuda-nvdisasm-10-0\n",
            "  cuda-nvdisasm-10-1 cuda-nvdisasm-11-0 cuda-nvml-dev-10-0 cuda-nvml-dev-10-1\n",
            "  cuda-nvml-dev-11-0 cuda-nvprof-10-0 cuda-nvprof-10-1 cuda-nvprof-11-0\n",
            "  cuda-nvprune-10-0 cuda-nvprune-10-1 cuda-nvprune-11-0 cuda-nvtx-10-0\n",
            "  cuda-nvtx-10-1 cuda-nvtx-11-0 cuda-nvvp-10-0 cuda-nvvp-10-1 cuda-nvvp-11-0\n",
            "  cuda-nvvp-11-1 cuda-samples-10-0 cuda-samples-10-1 cuda-samples-11-0\n",
            "  cuda-samples-11-1 cuda-sanitizer-11-0 cuda-sanitizer-api-10-1\n",
            "  cuda-toolkit-10-0 cuda-toolkit-10-1 cuda-toolkit-11-0 cuda-toolkit-11-1\n",
            "  cuda-tools-10-0 cuda-tools-10-1 cuda-tools-11-0 cuda-tools-11-1\n",
            "  cuda-visual-tools-10-0 cuda-visual-tools-10-1 cuda-visual-tools-11-0\n",
            "  cuda-visual-tools-11-1 default-jre dkms freeglut3-dev keyboard-configuration\n",
            "  libargon2-0 libcap2 libcryptsetup12 libdevmapper1.02.1 libidn11 libip4tc0\n",
            "  libjansson4 libnvidia-cfg1-510 libnvidia-common-460 libnvidia-common-510\n",
            "  libnvidia-extra-510 libnvidia-fbc1-510 libnvidia-gl-510 libpam-systemd\n",
            "  libpolkit-agent-1-0 libpolkit-backend-1-0 libpolkit-gobject-1-0 libxi-dev\n",
            "  libxmu-dev libxmu-headers libxnvctrl0 libxtst6 nsight-compute-2020.2.1\n",
            "  nsight-compute-2022.1.0 nsight-systems-2020.3.2 nsight-systems-2020.3.4\n",
            "  nsight-systems-2021.5.2 nvidia-dkms-510 nvidia-kernel-common-510\n",
            "  nvidia-kernel-source-510 nvidia-modprobe nvidia-settings openjdk-11-jre\n",
            "  policykit-1 policykit-1-gnome python3-xkit screen-resolution-extra systemd\n",
            "  systemd-sysv udev xserver-xorg-core-hwe-18.04 xserver-xorg-video-nvidia-510\n",
            "Use 'apt autoremove' to remove them.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 39 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:3.4.8-0ubuntu0.2).\n",
            "The following packages were automatically installed and are no longer required:\n",
            "  cuda-command-line-tools-10-0 cuda-command-line-tools-10-1\n",
            "  cuda-command-line-tools-11-0 cuda-compiler-10-0 cuda-compiler-10-1\n",
            "  cuda-compiler-11-0 cuda-cuobjdump-10-0 cuda-cuobjdump-10-1\n",
            "  cuda-cuobjdump-11-0 cuda-cupti-10-0 cuda-cupti-10-1 cuda-cupti-11-0\n",
            "  cuda-cupti-dev-11-0 cuda-documentation-10-0 cuda-documentation-10-1\n",
            "  cuda-documentation-11-0 cuda-documentation-11-1 cuda-gdb-10-0 cuda-gdb-10-1\n",
            "  cuda-gdb-11-0 cuda-gpu-library-advisor-10-0 cuda-gpu-library-advisor-10-1\n",
            "  cuda-libraries-10-0 cuda-libraries-10-1 cuda-libraries-11-0\n",
            "  cuda-memcheck-10-0 cuda-memcheck-10-1 cuda-memcheck-11-0 cuda-nsight-10-0\n",
            "  cuda-nsight-10-1 cuda-nsight-11-0 cuda-nsight-11-1 cuda-nsight-compute-10-0\n",
            "  cuda-nsight-compute-10-1 cuda-nsight-compute-11-0 cuda-nsight-compute-11-1\n",
            "  cuda-nsight-systems-10-1 cuda-nsight-systems-11-0 cuda-nsight-systems-11-1\n",
            "  cuda-nvcc-10-0 cuda-nvcc-10-1 cuda-nvcc-11-0 cuda-nvdisasm-10-0\n",
            "  cuda-nvdisasm-10-1 cuda-nvdisasm-11-0 cuda-nvml-dev-10-0 cuda-nvml-dev-10-1\n",
            "  cuda-nvml-dev-11-0 cuda-nvprof-10-0 cuda-nvprof-10-1 cuda-nvprof-11-0\n",
            "  cuda-nvprune-10-0 cuda-nvprune-10-1 cuda-nvprune-11-0 cuda-nvtx-10-0\n",
            "  cuda-nvtx-10-1 cuda-nvtx-11-0 cuda-nvvp-10-0 cuda-nvvp-10-1 cuda-nvvp-11-0\n",
            "  cuda-nvvp-11-1 cuda-samples-10-0 cuda-samples-10-1 cuda-samples-11-0\n",
            "  cuda-samples-11-1 cuda-sanitizer-11-0 cuda-sanitizer-api-10-1\n",
            "  cuda-toolkit-10-0 cuda-toolkit-10-1 cuda-toolkit-11-0 cuda-toolkit-11-1\n",
            "  cuda-tools-10-0 cuda-tools-10-1 cuda-tools-11-0 cuda-tools-11-1\n",
            "  cuda-visual-tools-10-0 cuda-visual-tools-10-1 cuda-visual-tools-11-0\n",
            "  cuda-visual-tools-11-1 default-jre dkms freeglut3-dev keyboard-configuration\n",
            "  libargon2-0 libcap2 libcryptsetup12 libdevmapper1.02.1 libidn11 libip4tc0\n",
            "  libjansson4 libnvidia-cfg1-510 libnvidia-common-460 libnvidia-common-510\n",
            "  libnvidia-extra-510 libnvidia-fbc1-510 libnvidia-gl-510 libpam-systemd\n",
            "  libpolkit-agent-1-0 libpolkit-backend-1-0 libpolkit-gobject-1-0 libxi-dev\n",
            "  libxmu-dev libxmu-headers libxnvctrl0 libxtst6 nsight-compute-2020.2.1\n",
            "  nsight-compute-2022.1.0 nsight-systems-2020.3.2 nsight-systems-2020.3.4\n",
            "  nsight-systems-2021.5.2 nvidia-dkms-510 nvidia-kernel-common-510\n",
            "  nvidia-kernel-source-510 nvidia-modprobe nvidia-settings openjdk-11-jre\n",
            "  policykit-1 policykit-1-gnome python3-xkit screen-resolution-extra systemd\n",
            "  systemd-sysv udev xserver-xorg-core-hwe-18.04 xserver-xorg-video-nvidia-510\n",
            "Use 'apt autoremove' to remove them.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 39 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "xvfb is already the newest version (2:1.19.6-1ubuntu4.10).\n",
            "The following packages were automatically installed and are no longer required:\n",
            "  cuda-command-line-tools-10-0 cuda-command-line-tools-10-1\n",
            "  cuda-command-line-tools-11-0 cuda-compiler-10-0 cuda-compiler-10-1\n",
            "  cuda-compiler-11-0 cuda-cuobjdump-10-0 cuda-cuobjdump-10-1\n",
            "  cuda-cuobjdump-11-0 cuda-cupti-10-0 cuda-cupti-10-1 cuda-cupti-11-0\n",
            "  cuda-cupti-dev-11-0 cuda-documentation-10-0 cuda-documentation-10-1\n",
            "  cuda-documentation-11-0 cuda-documentation-11-1 cuda-gdb-10-0 cuda-gdb-10-1\n",
            "  cuda-gdb-11-0 cuda-gpu-library-advisor-10-0 cuda-gpu-library-advisor-10-1\n",
            "  cuda-libraries-10-0 cuda-libraries-10-1 cuda-libraries-11-0\n",
            "  cuda-memcheck-10-0 cuda-memcheck-10-1 cuda-memcheck-11-0 cuda-nsight-10-0\n",
            "  cuda-nsight-10-1 cuda-nsight-11-0 cuda-nsight-11-1 cuda-nsight-compute-10-0\n",
            "  cuda-nsight-compute-10-1 cuda-nsight-compute-11-0 cuda-nsight-compute-11-1\n",
            "  cuda-nsight-systems-10-1 cuda-nsight-systems-11-0 cuda-nsight-systems-11-1\n",
            "  cuda-nvcc-10-0 cuda-nvcc-10-1 cuda-nvcc-11-0 cuda-nvdisasm-10-0\n",
            "  cuda-nvdisasm-10-1 cuda-nvdisasm-11-0 cuda-nvml-dev-10-0 cuda-nvml-dev-10-1\n",
            "  cuda-nvml-dev-11-0 cuda-nvprof-10-0 cuda-nvprof-10-1 cuda-nvprof-11-0\n",
            "  cuda-nvprune-10-0 cuda-nvprune-10-1 cuda-nvprune-11-0 cuda-nvtx-10-0\n",
            "  cuda-nvtx-10-1 cuda-nvtx-11-0 cuda-nvvp-10-0 cuda-nvvp-10-1 cuda-nvvp-11-0\n",
            "  cuda-nvvp-11-1 cuda-samples-10-0 cuda-samples-10-1 cuda-samples-11-0\n",
            "  cuda-samples-11-1 cuda-sanitizer-11-0 cuda-sanitizer-api-10-1\n",
            "  cuda-toolkit-10-0 cuda-toolkit-10-1 cuda-toolkit-11-0 cuda-toolkit-11-1\n",
            "  cuda-tools-10-0 cuda-tools-10-1 cuda-tools-11-0 cuda-tools-11-1\n",
            "  cuda-visual-tools-10-0 cuda-visual-tools-10-1 cuda-visual-tools-11-0\n",
            "  cuda-visual-tools-11-1 default-jre dkms freeglut3-dev keyboard-configuration\n",
            "  libargon2-0 libcap2 libcryptsetup12 libdevmapper1.02.1 libidn11 libip4tc0\n",
            "  libjansson4 libnvidia-cfg1-510 libnvidia-common-460 libnvidia-common-510\n",
            "  libnvidia-extra-510 libnvidia-fbc1-510 libnvidia-gl-510 libpam-systemd\n",
            "  libpolkit-agent-1-0 libpolkit-backend-1-0 libpolkit-gobject-1-0 libxi-dev\n",
            "  libxmu-dev libxmu-headers libxnvctrl0 libxtst6 nsight-compute-2020.2.1\n",
            "  nsight-compute-2022.1.0 nsight-systems-2020.3.2 nsight-systems-2020.3.4\n",
            "  nsight-systems-2021.5.2 nvidia-dkms-510 nvidia-kernel-common-510\n",
            "  nvidia-kernel-source-510 nvidia-modprobe nvidia-settings openjdk-11-jre\n",
            "  policykit-1 policykit-1-gnome python3-xkit screen-resolution-extra systemd\n",
            "  systemd-sysv udev xserver-xorg-core-hwe-18.04 xserver-xorg-video-nvidia-510\n",
            "Use 'apt autoremove' to remove them.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 39 not upgraded.\n",
            "Requirement already satisfied: pyvirtualdisplay in /usr/local/lib/python3.7/dist-packages (3.0)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (0.22.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.21.5)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym) (0.0.4)\n",
            "Requirement already satisfied: importlib-metadata>=4.10.0 in /usr/local/lib/python3.7/dist-packages (from gym) (4.11.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.10.0->gym) (3.7.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.10.0->gym) (3.10.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install box2d-py\n",
        "!pip3 install gym[Box_2D]\n",
        "!pip install pygame\n",
        "!pip install ale-py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DfSL-ia4HeW0",
        "outputId": "13b4c276-9ab3-4161-a60a-c30d37d07d2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: box2d-py in /usr/local/lib/python3.7/dist-packages (2.3.8)\n",
            "Requirement already satisfied: gym[Box_2D] in /usr/local/lib/python3.7/dist-packages (0.22.0)\n",
            "\u001b[33mWARNING: gym 0.22.0 does not provide the extra 'box_2d'\u001b[0m\n",
            "Requirement already satisfied: importlib-metadata>=4.10.0 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (4.11.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.3.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (0.0.4)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.21.5)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.10.0->gym[Box_2D]) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.10.0->gym[Box_2D]) (3.7.0)\n",
            "Requirement already satisfied: pygame in /usr/local/lib/python3.7/dist-packages (2.1.2)\n",
            "Requirement already satisfied: ale-py in /usr/local/lib/python3.7/dist-packages (0.7.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from ale-py) (1.21.5)\n",
            "Requirement already satisfied: importlib-metadata>=4.10.0 in /usr/local/lib/python3.7/dist-packages (from ale-py) (4.11.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from ale-py) (5.4.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.10.0->ale-py) (3.7.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.10.0->ale-py) (3.10.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import os\n",
        "import random\n",
        "from collections import deque\n",
        "from typing import Deque, Dict, List, Tuple\n",
        "\n",
        "import gym\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from IPython.display import clear_output\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "\n",
        "# download segment tree module\n",
        "if IN_COLAB:\n",
        "    !wget https://raw.githubusercontent.com/curt-park/rainbow-is-all-you-need/master/segment_tree.py\n",
        "\n",
        "from segment_tree import MinSegmentTree, SumSegmentTree"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qErDAcadHXOM",
        "outputId": "81e85b62-166e-4b9c-ca7b-79325c23e685"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-02-21 10:57:19--  https://raw.githubusercontent.com/curt-park/rainbow-is-all-you-need/master/segment_tree.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4283 (4.2K) [text/plain]\n",
            "Saving to: ‘segment_tree.py.1’\n",
            "\n",
            "\rsegment_tree.py.1     0%[                    ]       0  --.-KB/s               \rsegment_tree.py.1   100%[===================>]   4.18K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-02-21 10:57:19 (64.0 MB/s) - ‘segment_tree.py.1’ saved [4283/4283]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Replay Buffer"
      ],
      "metadata": {
        "id": "feyM3bt2HrGP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayBuffer:\n",
        "    \"\"\"A simple numpy replay buffer.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        obs_dim: int,\n",
        "        size: int,\n",
        "        batch_size: int = 32,\n",
        "        n_step: int = 1,\n",
        "        gamma: float = 0.99\n",
        "    ):\n",
        "        self.obs_buf = np.zeros([size, obs_dim], dtype=np.float32)\n",
        "        self.next_obs_buf = np.zeros([size, obs_dim], dtype=np.float32)\n",
        "        self.acts_buf = np.zeros([size], dtype=np.float32)\n",
        "        self.rews_buf = np.zeros([size], dtype=np.float32)\n",
        "        self.done_buf = np.zeros(size, dtype=np.float32)\n",
        "        self.max_size, self.batch_size = size, batch_size\n",
        "        self.ptr, self.size, = 0, 0\n",
        "\n",
        "        # for N-step Learning\n",
        "        self.n_step_buffer = deque(maxlen=n_step)\n",
        "        self.n_step = n_step\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def store(\n",
        "        self,\n",
        "        obs: np.ndarray,\n",
        "        act: np.ndarray,\n",
        "        rew: float,\n",
        "        next_obs: np.ndarray,\n",
        "        done: bool,\n",
        "    ) -> Tuple[np.ndarray, np.ndarray, float, np.ndarray, bool]:\n",
        "        transition = (obs, act, rew, next_obs, done)\n",
        "        self.n_step_buffer.append(transition)\n",
        "\n",
        "        # single step transition is not ready\n",
        "        if len(self.n_step_buffer) < self.n_step:\n",
        "            return ()\n",
        "\n",
        "        # make a n-step transition\n",
        "        rew, next_obs, done = self._get_n_step_info(\n",
        "            self.n_step_buffer, self.gamma\n",
        "        )\n",
        "        obs, act = self.n_step_buffer[0][:2]\n",
        "\n",
        "        self.obs_buf[self.ptr] = obs\n",
        "        self.next_obs_buf[self.ptr] = next_obs\n",
        "        self.acts_buf[self.ptr] = act\n",
        "        self.rews_buf[self.ptr] = rew\n",
        "        self.done_buf[self.ptr] = done\n",
        "        self.ptr = (self.ptr + 1) % self.max_size\n",
        "        self.size = min(self.size + 1, self.max_size)\n",
        "\n",
        "        return self.n_step_buffer[0]\n",
        "\n",
        "    def sample_batch(self) -> Dict[str, np.ndarray]:\n",
        "        idxs = np.random.choice(self.size, size=self.batch_size, replace=False)\n",
        "\n",
        "        return dict(\n",
        "            obs=self.obs_buf[idxs],\n",
        "            next_obs=self.next_obs_buf[idxs],\n",
        "            acts=self.acts_buf[idxs],\n",
        "            rews=self.rews_buf[idxs],\n",
        "            done=self.done_buf[idxs],\n",
        "            # for N-step Learning\n",
        "            indices=idxs,\n",
        "        )\n",
        "\n",
        "    def sample_batch_from_idxs(\n",
        "        self, idxs: np.ndarray\n",
        "    ) -> Dict[str, np.ndarray]:\n",
        "        # for N-step Learning\n",
        "        return dict(\n",
        "            obs=self.obs_buf[idxs],\n",
        "            next_obs=self.next_obs_buf[idxs],\n",
        "            acts=self.acts_buf[idxs],\n",
        "            rews=self.rews_buf[idxs],\n",
        "            done=self.done_buf[idxs],\n",
        "        )\n",
        "\n",
        "    def _get_n_step_info(\n",
        "        self, n_step_buffer: Deque, gamma: float\n",
        "    ) -> Tuple[np.int64, np.ndarray, bool]:\n",
        "        \"\"\"Return n step rew, next_obs, and done.\"\"\"\n",
        "        # info of the last transition\n",
        "        rew, next_obs, done = n_step_buffer[-1][-3:]\n",
        "\n",
        "        for transition in reversed(list(n_step_buffer)[:-1]):\n",
        "            r, n_o, d = transition[-3:]\n",
        "\n",
        "            rew = r + gamma * rew * (1 - d)\n",
        "            next_obs, done = (n_o, d) if d else (next_obs, done)\n",
        "\n",
        "        return rew, next_obs, done\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return self.size"
      ],
      "metadata": {
        "id": "IRmytzS1IO11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PrioritizedReplayBuffer(ReplayBuffer):\n",
        "    \"\"\"Prioritized Replay buffer.\n",
        "\n",
        "    Attributes:\n",
        "        max_priority (float): max priority\n",
        "        tree_ptr (int): next index of tree\n",
        "        alpha (float): alpha parameter for prioritized replay buffer\n",
        "        sum_tree (SumSegmentTree): sum tree for prior\n",
        "        min_tree (MinSegmentTree): min tree for min prior to get max weight\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        obs_dim: int,\n",
        "        size: int,\n",
        "        batch_size: int = 32,\n",
        "        alpha: float = 0.6,\n",
        "        n_step: int = 1,\n",
        "        gamma: float = 0.99,\n",
        "    ):\n",
        "        \"\"\"Initialization.\"\"\"\n",
        "        assert alpha >= 0\n",
        "\n",
        "        super(PrioritizedReplayBuffer, self).__init__(\n",
        "            obs_dim, size, batch_size, n_step, gamma\n",
        "        )\n",
        "        self.max_priority, self.tree_ptr = 1.0, 0\n",
        "        self.alpha = alpha\n",
        "\n",
        "        # capacity must be positive and a power of 2.\n",
        "        tree_capacity = 1\n",
        "        while tree_capacity < self.max_size:\n",
        "            tree_capacity *= 2\n",
        "\n",
        "        self.sum_tree = SumSegmentTree(tree_capacity)\n",
        "        self.min_tree = MinSegmentTree(tree_capacity)\n",
        "\n",
        "    def store(\n",
        "        self,\n",
        "        obs: np.ndarray,\n",
        "        act: int,\n",
        "        rew: float,\n",
        "        next_obs: np.ndarray,\n",
        "        done: bool,\n",
        "    ) -> Tuple[np.ndarray, np.ndarray, float, np.ndarray, bool]:\n",
        "        \"\"\"Store experience and priority.\"\"\"\n",
        "        transition = super().store(obs, act, rew, next_obs, done)\n",
        "\n",
        "        if transition:\n",
        "            self.sum_tree[self.tree_ptr] = self.max_priority ** self.alpha\n",
        "            self.min_tree[self.tree_ptr] = self.max_priority ** self.alpha\n",
        "            self.tree_ptr = (self.tree_ptr + 1) % self.max_size\n",
        "\n",
        "        return transition\n",
        "\n",
        "    def sample_batch(self, beta: float = 0.4) -> Dict[str, np.ndarray]:\n",
        "        \"\"\"Sample a batch of experiences.\"\"\"\n",
        "        assert len(self) >= self.batch_size\n",
        "        assert beta > 0\n",
        "\n",
        "        indices = self._sample_proportional()\n",
        "\n",
        "        obs = self.obs_buf[indices]\n",
        "        next_obs = self.next_obs_buf[indices]\n",
        "        acts = self.acts_buf[indices]\n",
        "        rews = self.rews_buf[indices]\n",
        "        done = self.done_buf[indices]\n",
        "        weights = np.array([self._calculate_weight(i, beta) for i in indices])\n",
        "\n",
        "        return dict(\n",
        "            obs=obs,\n",
        "            next_obs=next_obs,\n",
        "            acts=acts,\n",
        "            rews=rews,\n",
        "            done=done,\n",
        "            weights=weights,\n",
        "            indices=indices,\n",
        "        )\n",
        "\n",
        "    def update_priorities(self, indices: List[int], priorities: np.ndarray):\n",
        "        \"\"\"Update priorities of sampled transitions.\"\"\"\n",
        "        assert len(indices) == len(priorities)\n",
        "\n",
        "        for idx, priority in zip(indices, priorities):\n",
        "            assert priority > 0\n",
        "            assert 0 <= idx < len(self)\n",
        "\n",
        "            self.sum_tree[idx] = priority ** self.alpha\n",
        "            self.min_tree[idx] = priority ** self.alpha\n",
        "\n",
        "            self.max_priority = max(self.max_priority, priority)\n",
        "\n",
        "    def _sample_proportional(self) -> List[int]:\n",
        "        \"\"\"Sample indices based on proportions.\"\"\"\n",
        "        indices = []\n",
        "        p_total = self.sum_tree.sum(0, len(self) - 1)\n",
        "        segment = p_total / self.batch_size\n",
        "\n",
        "        for i in range(self.batch_size):\n",
        "            a = segment * i\n",
        "            b = segment * (i + 1)\n",
        "            upperbound = random.uniform(a, b)\n",
        "            idx = self.sum_tree.retrieve(upperbound)\n",
        "            indices.append(idx)\n",
        "\n",
        "        return indices\n",
        "\n",
        "    def _calculate_weight(self, idx: int, beta: float):\n",
        "        \"\"\"Calculate the weight of the experience at idx.\"\"\"\n",
        "        # get max weight\n",
        "        p_min = self.min_tree.min() / self.sum_tree.sum()\n",
        "        max_weight = (p_min * len(self)) ** (-beta)\n",
        "\n",
        "        # calculate weights\n",
        "        p_sample = self.sum_tree[idx] / self.sum_tree.sum()\n",
        "        weight = (p_sample * len(self)) ** (-beta)\n",
        "        weight = weight / max_weight\n",
        "\n",
        "        return weight"
      ],
      "metadata": {
        "id": "BuAWnFbBImXQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Noisy Layer"
      ],
      "metadata": {
        "id": "JE7avOE-KCGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NoisyLinear(nn.Module):\n",
        "    \"\"\"Noisy linear module for NoisyNet.\n",
        "\n",
        "\n",
        "\n",
        "    Attributes:\n",
        "        in_features (int): input size of linear module\n",
        "        out_features (int): output size of linear module\n",
        "        std_init (float): initial std value\n",
        "        weight_mu (nn.Parameter): mean value weight parameter\n",
        "        weight_sigma (nn.Parameter): std value weight parameter\n",
        "        bias_mu (nn.Parameter): mean value bias parameter\n",
        "        bias_sigma (nn.Parameter): std value bias parameter\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_features: int,\n",
        "        out_features: int,\n",
        "        std_init: float = 0.5,\n",
        "    ):\n",
        "        \"\"\"Initialization.\"\"\"\n",
        "        super(NoisyLinear, self).__init__()\n",
        "\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.std_init = std_init\n",
        "\n",
        "        self.weight_mu = nn.Parameter(torch.Tensor(out_features, in_features))\n",
        "        self.weight_sigma = nn.Parameter(\n",
        "            torch.Tensor(out_features, in_features)\n",
        "        )\n",
        "        self.register_buffer(\n",
        "            \"weight_epsilon\", torch.Tensor(out_features, in_features)\n",
        "        )\n",
        "\n",
        "        self.bias_mu = nn.Parameter(torch.Tensor(out_features))\n",
        "        self.bias_sigma = nn.Parameter(torch.Tensor(out_features))\n",
        "        self.register_buffer(\"bias_epsilon\", torch.Tensor(out_features))\n",
        "\n",
        "        self.reset_parameters()\n",
        "        self.reset_noise()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        \"\"\"Reset trainable network parameters (factorized gaussian noise).\"\"\"\n",
        "        mu_range = 1 / math.sqrt(self.in_features)\n",
        "        self.weight_mu.data.uniform_(-mu_range, mu_range)\n",
        "        self.weight_sigma.data.fill_(\n",
        "            self.std_init / math.sqrt(self.in_features)\n",
        "        )\n",
        "        self.bias_mu.data.uniform_(-mu_range, mu_range)\n",
        "        self.bias_sigma.data.fill_(\n",
        "            self.std_init / math.sqrt(self.out_features)\n",
        "        )\n",
        "\n",
        "    def reset_noise(self):\n",
        "        \"\"\"Make new noise.\"\"\"\n",
        "        epsilon_in = self.scale_noise(self.in_features)\n",
        "        epsilon_out = self.scale_noise(self.out_features)\n",
        "\n",
        "        # outer product\n",
        "        self.weight_epsilon.copy_(epsilon_out.ger(epsilon_in))\n",
        "        self.bias_epsilon.copy_(epsilon_out)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Forward method implementation.\n",
        "\n",
        "        We don't use separate statements on train / eval mode.\n",
        "        It doesn't show remarkable difference of performance.\n",
        "        \"\"\"\n",
        "        return F.linear(\n",
        "            x,\n",
        "            self.weight_mu + self.weight_sigma * self.weight_epsilon,\n",
        "            self.bias_mu + self.bias_sigma * self.bias_epsilon,\n",
        "        )\n",
        "\n",
        "    @staticmethod\n",
        "    def scale_noise(size: int) -> torch.Tensor:\n",
        "        \"\"\"Set scale to make noise (factorized gaussian noise).\"\"\"\n",
        "        x = torch.randn(size)\n",
        "\n",
        "        return x.sign().mul(x.abs().sqrt())"
      ],
      "metadata": {
        "id": "woRqaXL6KFFu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Networks"
      ],
      "metadata": {
        "id": "jOaPm5wPKIRP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Network(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_dim: int,\n",
        "        out_dim: int,\n",
        "        atom_size: int,\n",
        "        support: torch.Tensor\n",
        "    ):\n",
        "        \"\"\"Initialization.\"\"\"\n",
        "        super(Network, self).__init__()\n",
        "\n",
        "        self.support = support\n",
        "        self.out_dim = out_dim\n",
        "        self.atom_size = atom_size\n",
        "\n",
        "        # set common feature layer\n",
        "        self.feature_layer = nn.Sequential(\n",
        "            nn.Linear(in_dim, 1024),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        # set advantage layer\n",
        "        self.advantage_hidden_layer = NoisyLinear(1024, 1024)\n",
        "        self.advantage_layer = NoisyLinear(1024, out_dim * atom_size)\n",
        "\n",
        "        # set value layer\n",
        "        self.value_hidden_layer = NoisyLinear(1024, 1024)\n",
        "        self.value_layer = NoisyLinear(1024, atom_size)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Forward method implementation.\"\"\"\n",
        "        dist = self.dist(x)\n",
        "        q = torch.sum(dist * self.support, dim=2)\n",
        "\n",
        "        return q\n",
        "\n",
        "    def dist(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Get distribution for atoms.\"\"\"\n",
        "        feature = self.feature_layer(x)\n",
        "        adv_hid = F.relu(self.advantage_hidden_layer(feature))\n",
        "        val_hid = F.relu(self.value_hidden_layer(feature))\n",
        "\n",
        "        advantage = self.advantage_layer(adv_hid).view(\n",
        "            -1, self.out_dim, self.atom_size\n",
        "        )\n",
        "        value = self.value_layer(val_hid).view(-1, 1, self.atom_size)\n",
        "        q_atoms = value + advantage - advantage.mean(dim=1, keepdim=True)\n",
        "\n",
        "        dist = F.softmax(q_atoms, dim=-1)\n",
        "        dist = dist.clamp(min=1e-3)  # for avoiding nans\n",
        "\n",
        "        return dist\n",
        "\n",
        "    def reset_noise(self):\n",
        "        \"\"\"Reset all noisy layers.\"\"\"\n",
        "        self.advantage_hidden_layer.reset_noise()\n",
        "        self.advantage_layer.reset_noise()\n",
        "        self.value_hidden_layer.reset_noise()\n",
        "        self.value_layer.reset_noise()"
      ],
      "metadata": {
        "id": "DLXdZgNrKH0H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Agent"
      ],
      "metadata": {
        "id": "mfwdEHnXKfkZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DQNAgent:\n",
        "    \"\"\"DQN Agent interacting with environment.\n",
        "\n",
        "    Attribute:\n",
        "        env (gym.Env): openAI Gym environment\n",
        "        memory (PrioritizedReplayBuffer): replay memory to store transitions\n",
        "        batch_size (int): batch size for sampling\n",
        "        target_update (int): period for target model's hard update\n",
        "        gamma (float): discount factor\n",
        "        dqn (Network): model to train and select actions\n",
        "        dqn_target (Network): target model to update\n",
        "        optimizer (torch.optim): optimizer for training dqn\n",
        "        transition (list): transition information including\n",
        "                           state, action, reward, next_state, done\n",
        "        v_min (float): min value of support\n",
        "        v_max (float): max value of support\n",
        "        atom_size (int): the unit number of support\n",
        "        support (torch.Tensor): support for categorical dqn\n",
        "        use_n_step (bool): whether to use n_step memory\n",
        "        n_step (int): step number to calculate n-step td error\n",
        "        memory_n (ReplayBuffer): n-step replay buffer\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        env: gym.Env,\n",
        "        memory_size: int,\n",
        "        batch_size: int,\n",
        "        target_update: int,\n",
        "        gamma: float = 0.99,\n",
        "        # PER parameters\n",
        "        alpha: float = 0.2,\n",
        "        beta: float = 0.6,\n",
        "        prior_eps: float = 1e-6,\n",
        "        # Categorical DQN parameters\n",
        "        v_min: float = 0.0,\n",
        "        v_max: float = 200.0,\n",
        "        atom_size: int = 51,\n",
        "        # N-step Learning\n",
        "        n_step: int = 3,\n",
        "    ):\n",
        "        \"\"\"Initialization.\n",
        "\n",
        "        Args:\n",
        "            env (gym.Env): openAI Gym environment\n",
        "            memory_size (int): length of memory\n",
        "            batch_size (int): batch size for sampling\n",
        "            target_update (int): period for target model's hard update\n",
        "            lr (float): learning rate\n",
        "            gamma (float): discount factor\n",
        "            alpha (float): determines how much prioritization is used\n",
        "            beta (float): determines how much importance sampling is used\n",
        "            prior_eps (float): guarantees every transition can be sampled\n",
        "            v_min (float): min value of support\n",
        "            v_max (float): max value of support\n",
        "            atom_size (int): the unit number of support\n",
        "            n_step (int): step number to calculate n-step td error\n",
        "        \"\"\"\n",
        "        obs_dim = env.observation_space.shape[0]\n",
        "        action_dim = env.action_space.n\n",
        "\n",
        "        self.env = env\n",
        "        self.batch_size = batch_size\n",
        "        self.target_update = target_update\n",
        "        self.gamma = gamma\n",
        "        # NoisyNet: All attributes related to epsilon are removed\n",
        "\n",
        "        # device: cpu / gpu\n",
        "        self.device = torch.device(\n",
        "            \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        )\n",
        "        print(self.device)\n",
        "\n",
        "        # PER\n",
        "        # memory for 1-step Learning\n",
        "        self.beta = beta\n",
        "        self.prior_eps = prior_eps\n",
        "        self.memory = PrioritizedReplayBuffer(\n",
        "            obs_dim, memory_size, batch_size, alpha=alpha\n",
        "        )\n",
        "\n",
        "        # memory for N-step Learning\n",
        "        self.use_n_step = True if n_step > 1 else False\n",
        "        if self.use_n_step:\n",
        "            self.n_step = n_step\n",
        "            self.memory_n = ReplayBuffer(\n",
        "                obs_dim, memory_size, batch_size, n_step=n_step, gamma=gamma\n",
        "            )\n",
        "\n",
        "        # Categorical DQN parameters\n",
        "        self.v_min = v_min\n",
        "        self.v_max = v_max\n",
        "        self.atom_size = atom_size\n",
        "        self.support = torch.linspace(\n",
        "            self.v_min, self.v_max, self.atom_size\n",
        "        ).to(self.device)\n",
        "\n",
        "        # networks: dqn, dqn_target\n",
        "        self.dqn = Network(\n",
        "            obs_dim, action_dim, self.atom_size, self.support\n",
        "        ).to(self.device)\n",
        "        self.dqn_target = Network(\n",
        "            obs_dim, action_dim, self.atom_size, self.support\n",
        "        ).to(self.device)\n",
        "        self.dqn_target.load_state_dict(self.dqn.state_dict())\n",
        "        self.dqn_target.eval()\n",
        "\n",
        "        # optimizer\n",
        "        self.optimizer = optim.Adam(self.dqn.parameters())\n",
        "\n",
        "        # transition to store in memory\n",
        "        self.transition = list()\n",
        "\n",
        "        # mode: train / test\n",
        "        self.is_test = False\n",
        "\n",
        "    def select_action(self, state: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Select an action from the input state.\"\"\"\n",
        "        # NoisyNet: no epsilon greedy action selection\n",
        "        selected_action = self.dqn(\n",
        "            torch.FloatTensor(state).to(self.device)\n",
        "        ).argmax()\n",
        "        selected_action = selected_action.detach().cpu().numpy()\n",
        "\n",
        "        if not self.is_test:\n",
        "            self.transition = [state, selected_action]\n",
        "\n",
        "        return selected_action\n",
        "\n",
        "    def step(self, action: np.ndarray) -> Tuple[np.ndarray, np.float64, bool]:\n",
        "        \"\"\"Take an action and return the response of the env.\"\"\"\n",
        "        next_state, reward, done, _ = self.env.step(action)\n",
        "\n",
        "        if not self.is_test:\n",
        "            self.transition += [reward, next_state, done]\n",
        "\n",
        "            # N-step transition\n",
        "            if self.use_n_step:\n",
        "                one_step_transition = self.memory_n.store(*self.transition)\n",
        "            # 1-step transition\n",
        "            else:\n",
        "                one_step_transition = self.transition\n",
        "\n",
        "            # add a single step transition\n",
        "            if one_step_transition:\n",
        "                self.memory.store(*one_step_transition)\n",
        "\n",
        "        return next_state, reward, done\n",
        "\n",
        "    def update_model(self) -> torch.Tensor:\n",
        "        \"\"\"Update the model by gradient descent.\"\"\"\n",
        "        # PER needs beta to calculate weights\n",
        "        samples = self.memory.sample_batch(self.beta)\n",
        "        weights = torch.FloatTensor(\n",
        "            samples[\"weights\"].reshape(-1, 1)\n",
        "        ).to(self.device)\n",
        "        indices = samples[\"indices\"]\n",
        "\n",
        "        # 1-step Learning loss\n",
        "        elementwise_loss = self._compute_dqn_loss(samples, self.gamma)\n",
        "\n",
        "        # PER: importance sampling before average\n",
        "        loss = torch.mean(elementwise_loss * weights)\n",
        "\n",
        "        # N-step Learning loss\n",
        "        # we are gonna combine 1-step loss and n-step loss so as to\n",
        "        # prevent high-variance. The original rainbow employs n-step loss only.\n",
        "        if self.use_n_step:\n",
        "            gamma = self.gamma ** self.n_step\n",
        "            samples = self.memory_n.sample_batch_from_idxs(indices)\n",
        "            elementwise_loss_n_loss = self._compute_dqn_loss(samples, gamma)\n",
        "            elementwise_loss += elementwise_loss_n_loss\n",
        "\n",
        "            # PER: importance sampling before average\n",
        "            loss = torch.mean(elementwise_loss * weights)\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        clip_grad_norm_(self.dqn.parameters(), 10.0)\n",
        "        self.optimizer.step()\n",
        "\n",
        "        # PER: update priorities\n",
        "        loss_for_prior = elementwise_loss.detach().cpu().numpy()\n",
        "        new_priorities = loss_for_prior + self.prior_eps\n",
        "        self.memory.update_priorities(indices, new_priorities)\n",
        "\n",
        "        # NoisyNet: reset noise\n",
        "        self.dqn.reset_noise()\n",
        "        self.dqn_target.reset_noise()\n",
        "\n",
        "        return loss.item()\n",
        "\n",
        "    def train(self, num_frames: int, plotting_interval: int = 200):\n",
        "        \"\"\"Train the agent.\"\"\"\n",
        "        self.is_test = False\n",
        "\n",
        "        state = self.env.reset()\n",
        "        update_cnt = 0\n",
        "        losses = []\n",
        "        scores = []\n",
        "        score = 0\n",
        "\n",
        "        for frame_idx in range(1, num_frames + 1):\n",
        "            action = self.select_action(state)\n",
        "            next_state, reward, done = self.step(action)\n",
        "\n",
        "            state = next_state\n",
        "            score += reward\n",
        "\n",
        "            # NoisyNet: removed decrease of epsilon\n",
        "\n",
        "            # PER: increase beta\n",
        "            fraction = min(frame_idx / num_frames, 1.0)\n",
        "            self.beta = self.beta + fraction * (1.0 - self.beta)\n",
        "\n",
        "            # if episode ends\n",
        "            if done:\n",
        "                state = self.env.reset()\n",
        "                scores.append(score)\n",
        "                score = 0\n",
        "\n",
        "            # if training is ready\n",
        "            if len(self.memory) >= self.batch_size:\n",
        "                loss = self.update_model()\n",
        "                losses.append(loss)\n",
        "                update_cnt += 1\n",
        "\n",
        "                # if hard update is needed\n",
        "                if update_cnt % self.target_update == 0:\n",
        "                    self._target_hard_update()\n",
        "\n",
        "            # plotting\n",
        "            if frame_idx % plotting_interval == 0:\n",
        "                self._plot(frame_idx, scores, losses)\n",
        "\n",
        "        self.env.close()\n",
        "\n",
        "    def test(self, video_folder: str) -> None:\n",
        "        \"\"\"Test the agent.\"\"\"\n",
        "        self.is_test = True\n",
        "\n",
        "        # for recording a video\n",
        "        naive_env = self.env\n",
        "        self.env = gym.wrappers.RecordVideo(self.env, video_folder=video_folder)\n",
        "\n",
        "        state = self.env.reset()\n",
        "        done = False\n",
        "        score = 0\n",
        "\n",
        "        while not done:\n",
        "            action = self.select_action(state)\n",
        "            next_state, reward, done = self.step(action)\n",
        "\n",
        "            state = next_state\n",
        "            score += reward\n",
        "\n",
        "        print(\"score: \", score)\n",
        "        self.env.close()\n",
        "\n",
        "        # reset\n",
        "        self.env = naive_env\n",
        "\n",
        "    def _compute_dqn_loss(self, samples: Dict[str, np.ndarray], gamma: float) -> torch.Tensor:\n",
        "        \"\"\"Return categorical dqn loss.\"\"\"\n",
        "        device = self.device  # for shortening the following lines\n",
        "        state = torch.FloatTensor(samples[\"obs\"]).to(device)\n",
        "        next_state = torch.FloatTensor(samples[\"next_obs\"]).to(device)\n",
        "        action = torch.LongTensor(samples[\"acts\"]).to(device)\n",
        "        reward = torch.FloatTensor(samples[\"rews\"].reshape(-1, 1)).to(device)\n",
        "        done = torch.FloatTensor(samples[\"done\"].reshape(-1, 1)).to(device)\n",
        "\n",
        "        # Categorical DQN algorithm\n",
        "        delta_z = float(self.v_max - self.v_min) / (self.atom_size - 1)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # Double DQN\n",
        "            next_action = self.dqn(next_state).argmax(1)\n",
        "            next_dist = self.dqn_target.dist(next_state)\n",
        "            next_dist = next_dist[range(self.batch_size), next_action]\n",
        "\n",
        "            t_z = reward + (1 - done) * gamma * self.support\n",
        "            t_z = t_z.clamp(min=self.v_min, max=self.v_max)\n",
        "            b = (t_z - self.v_min) / delta_z\n",
        "            l = b.floor().long()\n",
        "            u = b.ceil().long()\n",
        "\n",
        "            offset = (\n",
        "                torch.linspace(\n",
        "                    0, (self.batch_size - 1) * self.atom_size, self.batch_size\n",
        "                ).long()\n",
        "                .unsqueeze(1)\n",
        "                .expand(self.batch_size, self.atom_size)\n",
        "                .to(self.device)\n",
        "            )\n",
        "\n",
        "            proj_dist = torch.zeros(next_dist.size(), device=self.device)\n",
        "            proj_dist.view(-1).index_add_(\n",
        "                0, (l + offset).view(-1), (next_dist * (u.float() - b)).view(-1)\n",
        "            )\n",
        "            proj_dist.view(-1).index_add_(\n",
        "                0, (u + offset).view(-1), (next_dist * (b - l.float())).view(-1)\n",
        "            )\n",
        "\n",
        "        dist = self.dqn.dist(state)\n",
        "        log_p = torch.log(dist[range(self.batch_size), action])\n",
        "        elementwise_loss = -(proj_dist * log_p).sum(1)\n",
        "\n",
        "        return elementwise_loss\n",
        "\n",
        "    def _target_hard_update(self):\n",
        "        \"\"\"Hard update: target <- local.\"\"\"\n",
        "        self.dqn_target.load_state_dict(self.dqn.state_dict())\n",
        "\n",
        "    def _plot(\n",
        "        self,\n",
        "        frame_idx: int,\n",
        "        scores: List[float],\n",
        "        losses: List[float],\n",
        "    ):\n",
        "        \"\"\"Plot the training progresses.\"\"\"\n",
        "        clear_output(True)\n",
        "        plt.figure(figsize=(20, 5))\n",
        "        plt.subplot(131)\n",
        "        plt.title('frame %s. score: %s' % (frame_idx, np.mean(scores[-10:])))\n",
        "        plt.plot(scores)\n",
        "        plt.subplot(132)\n",
        "        plt.title('loss')\n",
        "        plt.plot(losses)\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "AHSHXR31Kel2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run"
      ],
      "metadata": {
        "id": "uYZS8hXLKsEf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# environment\n",
        "env_id = \"LunarLander-v2\"\n",
        "env = gym.make(env_id)\n",
        "if IN_COLAB:\n",
        "    env = gym.wrappers.Monitor(env, \"videos\", force=True)"
      ],
      "metadata": {
        "id": "natIEBxYKuhg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 68\n",
        "\n",
        "def seed_torch(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.backends.cudnn.enabled:\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "seed_torch(seed)\n",
        "env.seed(seed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6gBVvh_EKzPC",
        "outputId": "f658b4c2-6463-4269-b118-d6f271a595ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[68]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# parameters\n",
        "num_frames = 50000\n",
        "memory_size = 100000\n",
        "batch_size = 256\n",
        "target_update = 100\n",
        "\n",
        "# train\n",
        "agent = DQNAgent(env, memory_size, batch_size, target_update)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x7CS7WBZK1UH",
        "outputId": "0444cc4c-f43e-44d7-daec-c4b1da64f2da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent.train(num_frames)"
      ],
      "metadata": {
        "id": "RcPJz5BBK38N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate"
      ],
      "metadata": {
        "id": "L0pW07a3K8wD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "video_folder=\"/content/drive/MyDrive/RL_Team/rainbow\"\n",
        "agent.test(video_folder=video_folder)"
      ],
      "metadata": {
        "id": "udeQvEHRK-Jo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Folder"
      ],
      "metadata": {
        "id": "iMyGrsYFK_CA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import base64\n",
        "import glob\n",
        "import io\n",
        "import os\n",
        "\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "\n",
        "def ipython_show_video(path: str) -> None:\n",
        "    \"\"\"Show a video at `path` within IPython Notebook.\"\"\"\n",
        "    if not os.path.isfile(path):\n",
        "        raise NameError(\"Cannot access: {}\".format(path))\n",
        "\n",
        "    video = io.open(path, \"r+b\").read()\n",
        "    encoded = base64.b64encode(video)\n",
        "\n",
        "    display(HTML(\n",
        "        data=\"\"\"\n",
        "        <video width=\"320\" height=\"240\" alt=\"test\" controls>\n",
        "        <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\"/>\n",
        "        </video>\n",
        "        \"\"\".format(encoded.decode(\"ascii\"))\n",
        "    ))\n",
        "\n",
        "\n",
        "def show_latest_video(video_folder: str) -> str:\n",
        "    \"\"\"Show the most recently recorded video from video folder.\"\"\"\n",
        "    list_of_files = glob.glob(os.path.join(video_folder, \"*.mp4\"))\n",
        "    latest_file = max(list_of_files, key=os.path.getctime)\n",
        "    ipython_show_video(latest_file)\n",
        "    return latest_file\n",
        "\n",
        "\n",
        "latest_file = show_latest_video(video_folder=video_folder)\n",
        "print(\"Played:\", latest_file)"
      ],
      "metadata": {
        "id": "yNBGUlJdLAPg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}