{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1vQQ2Kf1gQB"
      },
      "source": [
        "### ONLY RUN THIS CELL IF THESE DEPENDENCIES ARE NOT YET INSTALLED ON YOUR DEVICE\n",
        "\n",
        "If you get some error saying XXX can't be found, then just run this\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, sys\n",
        "from google.colab import drive\n",
        "\n",
        "#Mount your Google drive to the VM\n",
        "drive.mount('/content/gdrive')\n",
        "sys.path.append('/content/gdrive/My Drive/RL_Team')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3fENd0HbMf0",
        "outputId": "120ada62-16a6-40a9-efe9-ede9a8ed1477"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l42lCbE61fOu"
      },
      "source": [
        "# Rendering Dependencies\n",
        "!pip install gym==0.19.0 pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "\n",
        "# ATARI Dependencies\n",
        "!apt-get update > /dev/null 2>&1\n",
        "!apt-get install cmake > /dev/null 2>&1\n",
        "!pip install --upgrade setuptools 2>&1\n",
        "!pip install ez_setup > /dev/null 2>&1\n",
        "!pip install gym[atari] > /dev/null 2>&1\n",
        "#!pip install --upgrade gym 2>&1\n",
        "\n",
        "!sudo apt-get install -y xvfb ffmpeg x11-utils\n",
        "\n",
        "!pip install -q 'imageio==2.4.0'\n",
        "!pip install -q PILLOW\n",
        "!pip install -q 'pyglet==1.3.2'\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"# Rendering Dependencies\n",
        "!pip install --target=$nb_path gym==0.19.0 pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install --target=$nb_path -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "\n",
        "# ATARI Dependencies\n",
        "!apt-get update > /dev/null 2>&1\n",
        "!apt-get install cmake > /dev/null 2>&1\n",
        "!pip install --target=$nb_path --upgrade setuptools 2>&1\n",
        "!pip install --target=$nb_path ez_setup > /dev/null 2>&1\n",
        "!pip install --target=$nb_path gym[atari] > /dev/null 2>&1\n",
        "#!pip install --upgrade gym 2>&1\n",
        "\n",
        "!sudo apt-get install -y xvfb ffmpeg x11-utils\n",
        "\n",
        "!pip install --target=$nb_path -q 'imageio==2.4.0'\n",
        "!pip install --target=$nb_path -q PILLOW\n",
        "!pip install --target=$nb_path -q 'pyglet==1.3.2'\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "aLmY5kO1bt91",
        "outputId": "292aabaf-74ba-433a-dddd-2a426d7a7d10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"# Rendering Dependencies\\n!pip install --target=$nb_path gym==0.19.0 pyvirtualdisplay > /dev/null 2>&1\\n!apt-get install --target=$nb_path -y xvfb python-opengl ffmpeg > /dev/null 2>&1\\n\\n# ATARI Dependencies\\n!apt-get update > /dev/null 2>&1\\n!apt-get install cmake > /dev/null 2>&1\\n!pip install --target=$nb_path --upgrade setuptools 2>&1\\n!pip install --target=$nb_path ez_setup > /dev/null 2>&1\\n!pip install --target=$nb_path gym[atari] > /dev/null 2>&1\\n#!pip install --upgrade gym 2>&1\\n\\n!sudo apt-get install -y xvfb ffmpeg x11-utils\\n \\n!pip install --target=$nb_path -q 'imageio==2.4.0'\\n!pip install --target=$nb_path -q PILLOW\\n!pip install --target=$nb_path -q 'pyglet==1.3.2'\""
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCnY9o8e1pxY"
      },
      "source": [
        "## 1) Importing everything..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gInfLPxHt-Vr"
      },
      "source": [
        "''' First we are going to import all the necessary libraries and directories'''\n",
        "\n",
        "# Import the main bois: Gym and other standard libraries\n",
        "import gym\n",
        "from gym import spaces\n",
        "import numpy as np\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# Import all necessary torch libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as T\n",
        "import torch.autograd as autograd\n",
        "\n",
        "# Rendering Dependencies\n",
        "from collections import namedtuple\n",
        "from PIL import Image\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers import Monitor\n",
        "gymlogger.set_level(40) #error only\n",
        "import io\n",
        "import glob\n",
        "import base64\n",
        "from IPython.display import HTML # The main dude who's gonna create the visualization\n",
        "\n",
        "from IPython import display as ipythondisplay\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# Processing Dependencies\n",
        "from collections import deque\n",
        "import time\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def query_environment(name):\n",
        "    env = gym.make(name)\n",
        "    spec = gym.spec(name)\n",
        "    print(f\"Action Space: {env.action_space}\")\n",
        "    print(f\"Observation Space: {env.observation_space.shape}\")\n",
        "    print(f\"Max Episode Steps: {spec.max_episode_steps}\")\n",
        "    print(f\"Nondeterministic: {spec.nondeterministic}\")\n",
        "    print(f\"Reward Range: {env.reward_range}\")\n",
        "    print(f\"Reward Threshold: {spec.reward_threshold}\")"
      ],
      "metadata": {
        "id": "ap_qb3OCDigL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_environment(\"PongNoFrameskip-v4\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aaDx-1NQDlRs",
        "outputId": "7e6af789-0260-4510-ccba-2a8c726710c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Action Space: Discrete(6)\n",
            "Observation Space: (210, 160, 3)\n",
            "Max Episode Steps: 400000\n",
            "Nondeterministic: False\n",
            "Reward Range: (-inf, inf)\n",
            "Reward Threshold: None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "class NoisyLinear(nn.Module):\n",
        "    def __init__(self, in_features, out_features, use_cuda, std_init=0.4):\n",
        "        super(NoisyLinear, self).__init__()\n",
        "\n",
        "        self.use_cuda     = use_cuda\n",
        "        self.in_features  = in_features\n",
        "        self.out_features = out_features\n",
        "        self.std_init     = std_init\n",
        "\n",
        "        self.weight_mu    = nn.Parameter(torch.FloatTensor(out_features, in_features))\n",
        "        self.weight_sigma = nn.Parameter(torch.FloatTensor(out_features, in_features))\n",
        "        self.register_buffer('weight_epsilon', torch.FloatTensor(out_features, in_features))\n",
        "\n",
        "        self.bias_mu    = nn.Parameter(torch.FloatTensor(out_features))\n",
        "        self.bias_sigma = nn.Parameter(torch.FloatTensor(out_features))\n",
        "        self.register_buffer('bias_epsilon', torch.FloatTensor(out_features))\n",
        "\n",
        "        self.reset_parameters()\n",
        "        self.reset_noise()\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.use_cuda:\n",
        "            weight_epsilon = self.weight_epsilon.cuda()\n",
        "            bias_epsilon   = self.bias_epsilon.cuda()\n",
        "        else:\n",
        "            weight_epsilon = self.weight_epsilon\n",
        "            bias_epsilon   = self.bias_epsilon\n",
        "\n",
        "        if self.training:\n",
        "            weight = self.weight_mu + self.weight_sigma.mul(Variable(weight_epsilon))\n",
        "            bias   = self.bias_mu   + self.bias_sigma.mul(Variable(bias_epsilon))\n",
        "        else:\n",
        "            weight = self.weight_mu\n",
        "            bias   = self.bias_mu\n",
        "\n",
        "        return F.linear(x, weight, bias)\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        mu_range = 1 / math.sqrt(self.weight_mu.size(1))\n",
        "\n",
        "        self.weight_mu.data.uniform_(-mu_range, mu_range)\n",
        "        self.weight_sigma.data.fill_(self.std_init / math.sqrt(self.weight_sigma.size(1)))\n",
        "\n",
        "        self.bias_mu.data.uniform_(-mu_range, mu_range)\n",
        "        self.bias_sigma.data.fill_(self.std_init / math.sqrt(self.bias_sigma.size(0)))\n",
        "\n",
        "    def reset_noise(self):\n",
        "        epsilon_in  = self._scale_noise(self.in_features)\n",
        "        epsilon_out = self._scale_noise(self.out_features)\n",
        "\n",
        "        self.weight_epsilon.copy_(epsilon_out.ger(epsilon_in))\n",
        "        self.bias_epsilon.copy_(self._scale_noise(self.out_features))\n",
        "\n",
        "    def _scale_noise(self, size):\n",
        "        x = torch.randn(size)\n",
        "        x = x.sign().mul(x.abs().sqrt())\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "4LlSqvj_ZM54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "import operator\n",
        "\n",
        "\n",
        "class SegmentTree(object):\n",
        "    def __init__(self, capacity, operation, neutral_element):\n",
        "        \"\"\"Build a Segment Tree data structure.\n",
        "        https://en.wikipedia.org/wiki/Segment_tree\n",
        "        Can be used as regular array, but with two\n",
        "        important differences:\n",
        "            a) setting item's value is slightly slower.\n",
        "               It is O(lg capacity) instead of O(1).\n",
        "            b) user has access to an efficient `reduce`\n",
        "               operation which reduces `operation` over\n",
        "               a contiguous subsequence of items in the\n",
        "               array.\n",
        "        Paramters\n",
        "        ---------\n",
        "        capacity: int\n",
        "            Total size of the array - must be a power of two.\n",
        "        operation: lambda obj, obj -> obj\n",
        "            and operation for combining elements (eg. sum, max)\n",
        "            must for a mathematical group together with the set of\n",
        "            possible values for array elements.\n",
        "        neutral_element: obj\n",
        "            neutral element for the operation above. eg. float('-inf')\n",
        "            for max and 0 for sum.\n",
        "        \"\"\"\n",
        "        assert capacity > 0 and capacity & (capacity - 1) == 0, \"capacity must be positive and a power of 2.\"\n",
        "        self._capacity = capacity\n",
        "        self._value = [neutral_element for _ in range(2 * capacity)]\n",
        "        self._operation = operation\n",
        "\n",
        "    def _reduce_helper(self, start, end, node, node_start, node_end):\n",
        "        if start == node_start and end == node_end:\n",
        "            return self._value[node]\n",
        "        mid = (node_start + node_end) // 2\n",
        "        if end <= mid:\n",
        "            return self._reduce_helper(start, end, 2 * node, node_start, mid)\n",
        "        else:\n",
        "            if mid + 1 <= start:\n",
        "                return self._reduce_helper(start, end, 2 * node + 1, mid + 1, node_end)\n",
        "            else:\n",
        "                return self._operation(\n",
        "                    self._reduce_helper(start, mid, 2 * node, node_start, mid),\n",
        "                    self._reduce_helper(mid + 1, end, 2 * node + 1, mid + 1, node_end)\n",
        "                )\n",
        "\n",
        "    def reduce(self, start=0, end=None):\n",
        "        \"\"\"Returns result of applying `self.operation`\n",
        "        to a contiguous subsequence of the array.\n",
        "            self.operation(arr[start], operation(arr[start+1], operation(... arr[end])))\n",
        "        Parameters\n",
        "        ----------\n",
        "        start: int\n",
        "            beginning of the subsequence\n",
        "        end: int\n",
        "            end of the subsequences\n",
        "        Returns\n",
        "        -------\n",
        "        reduced: obj\n",
        "            result of reducing self.operation over the specified range of array elements.\n",
        "        \"\"\"\n",
        "        if end is None:\n",
        "            end = self._capacity\n",
        "        if end < 0:\n",
        "            end += self._capacity\n",
        "        end -= 1\n",
        "        return self._reduce_helper(start, end, 1, 0, self._capacity - 1)\n",
        "\n",
        "    def __setitem__(self, idx, val):\n",
        "        # index of the leaf\n",
        "        idx += self._capacity\n",
        "        self._value[idx] = val\n",
        "        idx //= 2\n",
        "        while idx >= 1:\n",
        "            self._value[idx] = self._operation(\n",
        "                self._value[2 * idx],\n",
        "                self._value[2 * idx + 1]\n",
        "            )\n",
        "            idx //= 2\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        assert 0 <= idx < self._capacity\n",
        "        return self._value[self._capacity + idx]\n",
        "\n",
        "\n",
        "class SumSegmentTree(SegmentTree):\n",
        "    def __init__(self, capacity):\n",
        "        super(SumSegmentTree, self).__init__(\n",
        "            capacity=capacity,\n",
        "            operation=operator.add,\n",
        "            neutral_element=0.0\n",
        "        )\n",
        "\n",
        "    def sum(self, start=0, end=None):\n",
        "        \"\"\"Returns arr[start] + ... + arr[end]\"\"\"\n",
        "        return super(SumSegmentTree, self).reduce(start, end)\n",
        "\n",
        "    def find_prefixsum_idx(self, prefixsum):\n",
        "        \"\"\"Find the highest index `i` in the array such that\n",
        "            sum(arr[0] + arr[1] + ... + arr[i - i]) <= prefixsum\n",
        "        if array values are probabilities, this function\n",
        "        allows to sample indexes according to the discrete\n",
        "        probability efficiently.\n",
        "        Parameters\n",
        "        ----------\n",
        "        perfixsum: float\n",
        "            upperbound on the sum of array prefix\n",
        "        Returns\n",
        "        -------\n",
        "        idx: int\n",
        "            highest index satisfying the prefixsum constraint\n",
        "        \"\"\"\n",
        "        assert 0 <= prefixsum <= self.sum() + 1e-5\n",
        "        idx = 1\n",
        "        while idx < self._capacity:  # while non-leaf\n",
        "            if self._value[2 * idx] > prefixsum:\n",
        "                idx = 2 * idx\n",
        "            else:\n",
        "                prefixsum -= self._value[2 * idx]\n",
        "                idx = 2 * idx + 1\n",
        "        return idx - self._capacity\n",
        "\n",
        "\n",
        "class MinSegmentTree(SegmentTree):\n",
        "    def __init__(self, capacity):\n",
        "        super(MinSegmentTree, self).__init__(\n",
        "            capacity=capacity,\n",
        "            operation=min,\n",
        "            neutral_element=float('inf')\n",
        "        )\n",
        "\n",
        "    def min(self, start=0, end=None):\n",
        "        \"\"\"Returns min(arr[start], ...,  arr[end])\"\"\"\n",
        "\n",
        "        return super(MinSegmentTree, self).reduce(start, end)\n",
        "\n",
        "\n",
        "class ReplayBuffer(object):\n",
        "    def __init__(self, size):\n",
        "        \"\"\"Create Replay buffer.\n",
        "        Parameters\n",
        "        ----------\n",
        "        size: int\n",
        "            Max number of transitions to store in the buffer. When the buffer\n",
        "            overflows the old memories are dropped.\n",
        "        \"\"\"\n",
        "        self._storage = []\n",
        "        self._maxsize = size\n",
        "        self._next_idx = 0\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._storage)\n",
        "\n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        data = (state, action, reward, next_state, done)\n",
        "\n",
        "        if self._next_idx >= len(self._storage):\n",
        "            self._storage.append(data)\n",
        "        else:\n",
        "            self._storage[self._next_idx] = data\n",
        "        self._next_idx = (self._next_idx + 1) % self._maxsize\n",
        "\n",
        "    def _encode_sample(self, idxes):\n",
        "        obses_t, actions, rewards, obses_tp1, dones = [], [], [], [], []\n",
        "        for i in idxes:\n",
        "            data = self._storage[i]\n",
        "            obs_t, action, reward, obs_tp1, done = data\n",
        "\n",
        "            obses_t.append(np.array(obs_t, copy=False))\n",
        "            actions.append(np.array(action, copy=False))\n",
        "            rewards.append(reward)\n",
        "            obses_tp1.append(np.array(obs_tp1, copy=False))\n",
        "            dones.append(done)\n",
        "        return np.array(obses_t), np.array(actions), np.array(rewards), np.array(obses_tp1), np.array(dones)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        \"\"\"Sample a batch of experiences.\n",
        "        Parameters\n",
        "        ----------\n",
        "        batch_size: int\n",
        "            How many transitions to sample.\n",
        "        Returns\n",
        "        -------\n",
        "        obs_batch: np.array\n",
        "            batch of observations\n",
        "        act_batch: np.array\n",
        "            batch of actions executed given obs_batch\n",
        "        rew_batch: np.array\n",
        "            rewards received as results of executing act_batch\n",
        "        next_obs_batch: np.array\n",
        "            next set of observations seen after executing act_batch\n",
        "        done_mask: np.array\n",
        "            done_mask[i] = 1 if executing act_batch[i] resulted in\n",
        "            the end of an episode and 0 otherwise.\n",
        "        \"\"\"\n",
        "        idxes = [random.randint(0, len(self._storage) - 1) for _ in range(batch_size)]\n",
        "        return self._encode_sample(idxes)\n",
        "\n",
        "\n",
        "class PrioritizedReplayBuffer(ReplayBuffer):\n",
        "    def __init__(self, size, alpha):\n",
        "        \"\"\"Create Prioritized Replay buffer.\n",
        "        Parameters\n",
        "        ----------\n",
        "        size: int\n",
        "            Max number of transitions to store in the buffer. When the buffer\n",
        "            overflows the old memories are dropped.\n",
        "        alpha: float\n",
        "            how much prioritization is used\n",
        "            (0 - no prioritization, 1 - full prioritization)\n",
        "        See Also\n",
        "        --------\n",
        "        ReplayBuffer.__init__\n",
        "        \"\"\"\n",
        "        super(PrioritizedReplayBuffer, self).__init__(size)\n",
        "        assert alpha > 0\n",
        "        self._alpha = alpha\n",
        "\n",
        "        it_capacity = 1\n",
        "        while it_capacity < size:\n",
        "            it_capacity *= 2\n",
        "\n",
        "        self._it_sum = SumSegmentTree(it_capacity)\n",
        "        self._it_min = MinSegmentTree(it_capacity)\n",
        "        self._max_priority = 1.0\n",
        "\n",
        "    def push(self, *args, **kwargs):\n",
        "        \"\"\"See ReplayBuffer.store_effect\"\"\"\n",
        "        idx = self._next_idx\n",
        "        super(PrioritizedReplayBuffer, self).push(*args, **kwargs)\n",
        "        self._it_sum[idx] = self._max_priority ** self._alpha\n",
        "        self._it_min[idx] = self._max_priority ** self._alpha\n",
        "\n",
        "    def _sample_proportional(self, batch_size):\n",
        "        res = []\n",
        "        for _ in range(batch_size):\n",
        "            # TODO(szymon): should we ensure no repeats?\n",
        "            mass = random.random() * self._it_sum.sum(0, len(self._storage) - 1)\n",
        "            idx = self._it_sum.find_prefixsum_idx(mass)\n",
        "            res.append(idx)\n",
        "        return res\n",
        "\n",
        "    def sample(self, batch_size, beta):\n",
        "        \"\"\"Sample a batch of experiences.\n",
        "        compared to ReplayBuffer.sample\n",
        "        it also returns importance weights and idxes\n",
        "        of sampled experiences.\n",
        "        Parameters\n",
        "        ----------\n",
        "        batch_size: int\n",
        "            How many transitions to sample.\n",
        "        beta: float\n",
        "            To what degree to use importance weights\n",
        "            (0 - no corrections, 1 - full correction)\n",
        "        Returns\n",
        "        -------\n",
        "        obs_batch: np.array\n",
        "            batch of observations\n",
        "        act_batch: np.array\n",
        "            batch of actions executed given obs_batch\n",
        "        rew_batch: np.array\n",
        "            rewards received as results of executing act_batch\n",
        "        next_obs_batch: np.array\n",
        "            next set of observations seen after executing act_batch\n",
        "        done_mask: np.array\n",
        "            done_mask[i] = 1 if executing act_batch[i] resulted in\n",
        "            the end of an episode and 0 otherwise.\n",
        "        weights: np.array\n",
        "            Array of shape (batch_size,) and dtype np.float32\n",
        "            denoting importance weight of each sampled transition\n",
        "        idxes: np.array\n",
        "            Array of shape (batch_size,) and dtype np.int32\n",
        "            idexes in buffer of sampled experiences\n",
        "        \"\"\"\n",
        "        assert beta > 0\n",
        "\n",
        "        idxes = self._sample_proportional(batch_size)\n",
        "\n",
        "        weights = []\n",
        "        p_min = self._it_min.min() / self._it_sum.sum()\n",
        "        max_weight = (p_min * len(self._storage)) ** (-beta)\n",
        "\n",
        "        for idx in idxes:\n",
        "            p_sample = self._it_sum[idx] / self._it_sum.sum()\n",
        "            weight = (p_sample * len(self._storage)) ** (-beta)\n",
        "            weights.append(weight / max_weight)\n",
        "        weights = np.array(weights)\n",
        "        encoded_sample = self._encode_sample(idxes)\n",
        "        return tuple(list(encoded_sample) + [weights, idxes])\n",
        "\n",
        "    def update_priorities(self, idxes, priorities):\n",
        "        \"\"\"Update priorities of sampled transitions.\n",
        "        sets priority of transition at index idxes[i] in buffer\n",
        "        to priorities[i].\n",
        "        Parameters\n",
        "        ----------\n",
        "        idxes: [int]\n",
        "            List of idxes of sampled transitions\n",
        "        priorities: [float]\n",
        "            List of updated priorities corresponding to\n",
        "            transitions at the sampled idxes denoted by\n",
        "            variable `idxes`.\n",
        "        \"\"\"\n",
        "        assert len(idxes) == len(priorities)\n",
        "        for idx, priority in zip(idxes, priorities):\n",
        "            assert priority > 0\n",
        "            assert 0 <= idx < len(self._storage)\n",
        "            self._it_sum[idx] = priority ** self._alpha\n",
        "            self._it_min[idx] = priority ** self._alpha\n",
        "\n",
        "            self._max_priority = max(self._max_priority, priority)"
      ],
      "metadata": {
        "id": "F1R8mHJuZYQn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"This file is used for specifying various schedules that evolve over\n",
        "time throughout the execution of the algorithm, such as:\n",
        " - learning rate for the optimizer\n",
        " - exploration epsilon for the epsilon greedy exploration strategy\n",
        " - beta parameter for beta parameter in prioritized replay\n",
        "\n",
        "Each schedule has a function `value(t)` which returns the current value\n",
        "of the parameter given the timestep t of the optimization procedure.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class Schedule(object):\n",
        "    def value(self, t):\n",
        "        \"\"\"Value of the schedule at time t\"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "\n",
        "class ConstantSchedule(object):\n",
        "    def __init__(self, value):\n",
        "        \"\"\"Value remains constant over time.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        value: float\n",
        "            Constant value of the schedule\n",
        "        \"\"\"\n",
        "        self._v = value\n",
        "\n",
        "    def value(self, t):\n",
        "        \"\"\"See Schedule.value\"\"\"\n",
        "        return self._v\n",
        "\n",
        "\n",
        "def linear_interpolation(l, r, alpha):\n",
        "    return l + alpha * (r - l)\n",
        "\n",
        "\n",
        "class PiecewiseSchedule(object):\n",
        "    def __init__(self, endpoints, interpolation=linear_interpolation, outside_value=None):\n",
        "        \"\"\"Piecewise schedule.\n",
        "\n",
        "        endpoints: [(int, int)]\n",
        "            list of pairs `(time, value)` meanining that schedule should output\n",
        "            `value` when `t==time`. All the values for time must be sorted in\n",
        "            an increasing order. When t is between two times, e.g. `(time_a, value_a)`\n",
        "            and `(time_b, value_b)`, such that `time_a <= t < time_b` then value outputs\n",
        "            `interpolation(value_a, value_b, alpha)` where alpha is a fraction of\n",
        "            time passed between `time_a` and `time_b` for time `t`.\n",
        "        interpolation: lambda float, float, float: float\n",
        "            a function that takes value to the left and to the right of t according\n",
        "            to the `endpoints`. Alpha is the fraction of distance from left endpoint to\n",
        "            right endpoint that t has covered. See linear_interpolation for example.\n",
        "        outside_value: float\n",
        "            if the value is requested outside of all the intervals sepecified in\n",
        "            `endpoints` this value is returned. If None then AssertionError is\n",
        "            raised when outside value is requested.\n",
        "        \"\"\"\n",
        "        idxes = [e[0] for e in endpoints]\n",
        "        assert idxes == sorted(idxes)\n",
        "        self._interpolation = interpolation\n",
        "        self._outside_value = outside_value\n",
        "        self._endpoints = endpoints\n",
        "\n",
        "    def value(self, t):\n",
        "        \"\"\"See Schedule.value\"\"\"\n",
        "        for (l_t, l), (r_t, r) in zip(self._endpoints[:-1], self._endpoints[1:]):\n",
        "            if l_t <= t and t < r_t:\n",
        "                alpha = float(t - l_t) / (r_t - l_t)\n",
        "                return self._interpolation(l, r, alpha)\n",
        "\n",
        "        # t does not belong to any of the pieces, so doom.\n",
        "        assert self._outside_value is not None\n",
        "        return self._outside_value\n",
        "\n",
        "\n",
        "class LinearSchedule(object):\n",
        "    def __init__(self, schedule_timesteps, final_p, initial_p=1.0):\n",
        "        \"\"\"Linear interpolation between initial_p and final_p over\n",
        "        schedule_timesteps. After this many timesteps pass final_p is\n",
        "        returned.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        schedule_timesteps: int\n",
        "            Number of timesteps for which to linearly anneal initial_p\n",
        "            to final_p\n",
        "        initial_p: float\n",
        "            initial output value\n",
        "        final_p: float\n",
        "            final output value\n",
        "        \"\"\"\n",
        "        self.schedule_timesteps = schedule_timesteps\n",
        "        self.final_p = final_p\n",
        "        self.initial_p = initial_p\n",
        "\n",
        "    def value(self, t):\n",
        "        \"\"\"See Schedule.value\"\"\"\n",
        "        fraction = min(float(t) / self.schedule_timesteps, 1.0)\n",
        "        return self.initial_p + fraction * (self.final_p - self.initial_p)"
      ],
      "metadata": {
        "id": "8Ab57HCdZd2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import deque\n",
        "import random\n",
        "from numpy import array\n",
        "\n",
        "\n",
        "class ReplayMemory(object):\n",
        "\n",
        "    def __init__(self, memory_size):\n",
        "        '''Setup the class variables required for the Replay Memory:\n",
        "            - The memory size\n",
        "            - The data structure where the experience tuples will be stored\n",
        "              (I suggest a deque)\n",
        "        '''\n",
        "        self.memory_size = memory_size\n",
        "        self.replay_memory_deque = deque([], maxlen = self.memory_size)\n",
        "\n",
        "    def sample_batch(self, batch_size=32):\n",
        "        '''\n",
        "        Take a random set of experiences from the Replay Memory deque and return the values from each experience\n",
        "        in their own respective arrays ->\n",
        "        return state, action, reward, done, next_state\n",
        "            - state: array of the state values from the random set of experiences\n",
        "            - action: array of the action values from the random set of experiences\n",
        "            - reward: array of the reward values from the random set of experiences\n",
        "            - done: array of the done values from the random set of experiences\n",
        "            - next_state: array of the next_state values from the random set of experiences\n",
        "        '''\n",
        "        state, action, reward, done, next_state = zip(*random.sample(self.replay_memory_deque, k = batch_size))\n",
        "        return array(state), array(action), array(reward), array(done), array(next_state)\n",
        "\n",
        "    # Push the data into the memory space\n",
        "    def append(self, state, action, reward, done, next_state):\n",
        "        '''\n",
        "        Append the data pased into the method from an experience into the deque\n",
        "        '''\n",
        "\n",
        "        self.replay_memory_deque.append((state, action, reward, done, next_state))\n",
        "\n",
        "    def __len__(self):\n",
        "        '''\n",
        "        Return the length of the deque\n",
        "        '''\n",
        "        return len(self.replay_memory_deque)"
      ],
      "metadata": {
        "id": "2Mt8SbuXD7kV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "class DQN(nn.Module):\n",
        "\n",
        "    def __init__(self, env, device):\n",
        "        super(DQN, self).__init__()\n",
        "\n",
        "        # Parameters that will be used later\n",
        "        self.input_dim = env.observation_space.shape\n",
        "        self.num_actions = env.action_space.n\n",
        "        self.device = device\n",
        "\n",
        "        '''\n",
        "        First begin by creating the CNN\n",
        "        I suggest building the CNN using the nn.Sequential() method.\n",
        "        If you're not sure what to do for the CNN, I would suggest taking a look over the following\n",
        "        paper which details working with the OpenAi Atari Gym.\n",
        "        https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf\n",
        "        '''\n",
        "        self.CNN = nn.Sequential(\n",
        "            nn.Conv2d(in_channels= self.input_dim[0], out_channels= 32, kernel_size= (8,8), stride= 4),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels= 32, out_channels= 64, kernel_size= (4,4), stride= 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels= 64, out_channels= 64, kernel_size= (3,3), stride= 1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Determine the fully connected layer's input size using the supplied CNN_output_dim() method\n",
        "        self.fcl_input_size = self.CNN_output_dim(self.input_dim)\n",
        "\n",
        "        '''\n",
        "        Now build the fully connected layer.\n",
        "        Again, I suggest building it with the nn.Sequential() method.\n",
        "        If you found the model architecture content in the paper listed above, then the following\n",
        "        architecture will be very easy.\n",
        "        '''\n",
        "        self.fcl = nn.Sequential(\n",
        "            nn.Linear(self.fcl_input_size, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, self.num_actions)\n",
        "        )\n",
        "\n",
        "    def CNN_output_dim(self, input_dim):\n",
        "        '''\n",
        "        This method is used to determine the output dimensions of the CNN (which is used\n",
        "        to set the input dimension of the fully connected layer).\n",
        "        '''\n",
        "        return self.CNN(torch.zeros(1, *input_dim)).flatten().shape[0]\n",
        "\n",
        "    def forward(self, X):\n",
        "        '''\n",
        "        This method is used to do a forward pass into the model, taking in the state and returning the Q value\n",
        "            1. Pass the state into the CNN\n",
        "            2. Flatten the output of the CNN using the flatten method with start_dim=1\n",
        "            3. Pass this flattened tensor into the fully connected layer\n",
        "            4. Return the output of the fully connected layer, i.e., the Q value\n",
        "        '''\n",
        "        X = self.CNN(X)\n",
        "        X = X.flatten(start_dim=1)\n",
        "        X = self.fcl(X)\n",
        "        return X\n",
        "\n",
        "    def get_action(self, state, epsilon):\n",
        "        '''\n",
        "        This methos is used to get the action that should be taken where the action will either be random (explore)\n",
        "        or calculated by the model (exploit).\n",
        "        '''\n",
        "        # Use random.random() here to generate a random float and if it's greater than epsilon, then exploit else explore\n",
        "        if random.uniform(0, 1) > epsilon:\n",
        "        # If exploiting, use the supplied epsilon_Greed_Strat() method to generate the action\n",
        "            new_action = self.epsilon_Greed_Strat(state)\n",
        "        # If exploring, use the random.randrange() method, passing in self.num_actions to generate the action\n",
        "        else:\n",
        "            new_action = random.randrange(self.num_actions)\n",
        "        # Then, return the action\n",
        "        return new_action\n",
        "\n",
        "    def epsilon_Greed_Strat(self, state):\n",
        "        '''\n",
        "        This method is used to determine the best action to take from a given state based on the previously-calculated\n",
        "        Q values.\n",
        "        '''\n",
        "        qval = self.get_qvals(state)\n",
        "        return qval.max(1)[1].data[0]\n",
        "\n",
        "    def get_qvals(self, state):\n",
        "        '''\n",
        "        This method is used to calculate the Q value of a given state, by passing the state into the model\n",
        "        '''\n",
        "        with torch.no_grad():\n",
        "            # Converting state data into tensor\n",
        "            state_t = torch.FloatTensor(np.float32(state)).unsqueeze(0).to(device=self.device)\n",
        "            return self.forward(state_t)\n",
        "\n",
        "    def calc_TD_Loss(self, batch_size, model, target_model, optimiser, experience, discount_factor):\n",
        "        \"\"\"The function will first take input from the exprience and then\n",
        "        calculate the optimum value from it. Later on, the Q-values are calculated\n",
        "        to give an idea of if it's converging to the optimal policy.\"\"\"\n",
        "\n",
        "        # Extract a sample of batches from our replay buffer (ReplayMemory class)\n",
        "        states, actions, rewards, dones, next_states = experience.sample_batch(batch_size)\n",
        "\n",
        "        # The following code is typical of any loss-calculation and training method.\n",
        "\n",
        "        # Convert all of our tuples to tensors:\n",
        "        states = torch.FloatTensor(np.float32(states)).to(self.device)\n",
        "        next_states = torch.FloatTensor(np.float32(next_states)).to(self.device)\n",
        "        rewards_t = torch.FloatTensor(rewards).to(self.device)\n",
        "        actions_t = torch.LongTensor(np.int64(actions)).to(self.device)\n",
        "        done_t = torch.FloatTensor(dones).to(self.device)\n",
        "\n",
        "        # Calculate the value-action function as well as the value-action function\n",
        "        # for the next state\n",
        "        qvals = model(states)\n",
        "        qvals = qvals.gather(1, actions_t.unsqueeze(1)).squeeze(1)\n",
        "\n",
        "\n",
        "        next_qvals = model(next_states)\n",
        "        next_qval_state = target_model(next_states)\n",
        "        next_qval = next_qval_state.gather(1, torch.max(next_qvals, 1)[1].unsqueeze(1)).squeeze(1)\n",
        "\n",
        "        # Calculate towards the optimum value-action function\n",
        "        expected_qvals = rewards_t + discount_factor * next_qval * (1 - done_t)\n",
        "\n",
        "        # Calculate loss with Mean-Square Loss Function:\n",
        "        loss = F.mse_loss(qvals, expected_qvals.detach().to(device=model.device))\n",
        "\n",
        "        # Backpropagate and update the model:\n",
        "        optimiser.zero_grad()\n",
        "        loss.backward()\n",
        "        optimiser.step()\n"
      ],
      "metadata": {
        "id": "WRmj1I5lD9-g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from gym.wrappers import Monitor\n",
        "import io\n",
        "import glob\n",
        "import base64\n",
        "from IPython.display import HTML    # The main dude who's gonna create the visualization\n",
        "from IPython import display as ipythondisplay\n",
        "from IPython.display import clear_output\n",
        "\n",
        "\n",
        "def video_display():\n",
        "    mp4Video = glob.glob('video/*.mp4')\n",
        "    if len(mp4Video) > 0:\n",
        "        mp4 = mp4Video[0]\n",
        "        video = io.open(mp4, 'r+b').read()\n",
        "        encoded = base64.b64encode(video)\n",
        "        ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay\n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "    else:\n",
        "        print(\"Could not find video\")\n",
        "\n",
        "\n",
        "def wrap_env_Video(env):\n",
        "    env = Monitor(env, './video', force=True)\n",
        "    return env\n",
        "\n",
        "\n",
        "def plot_save_Results(training_rewards, path):\n",
        "  clear_output(True)\n",
        "  plt.figure(figsize=(12,8))\n",
        "  plt.plot(training_rewards, label='Rewards')\n",
        "  plt.xlabel('Episodes')\n",
        "  plt.ylabel('Rewards')\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def save_weights(model, path, file_name=None):\n",
        "  if file_name is None:\n",
        "    file_name = 'your_models_trained_weights.pt'\n",
        "  weights_path = os.path.join(path,file_name)\n",
        "  torch.save(model.state_dict(), weights_path)\n",
        "\n",
        "\n",
        "def check_weights(path, file_name=None):\n",
        "  if file_name is None:\n",
        "    file_name = 'your_models_trained_weights.pt'\n",
        "  weights_path = os.path.join(path,file_name)\n",
        "  return os.path.exists(weights_path)"
      ],
      "metadata": {
        "id": "rmyMfI7wEDSX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent:\n",
        "    def __init__(self, environment, device, display_progress=True, fp=\"C:\\\\Users\\\\Chong Yen Juin\\\\Desktop\\\\IntroRL\\\\save_model\"):\n",
        "        self.environment = environment\n",
        "        self.fp = fp  # The file path we will be saving our weights to\n",
        "\n",
        "        # Hyperparameters - I strongly suggest you play around ith these to find better ones!\n",
        "        self.memory_size = 20000  # Maximum amount of experiences that can be stored\n",
        "        self.burn_in = 10000  # How many time steps should pass until the network learns (via TD Loss)\n",
        "        self.save_freq = 10  # How often the model gets saved to a .pt file, and the performance is graphed\n",
        "        self.max_episodes = 350  # The total number of episodes the agent will play in training\n",
        "        self.target_update_freq = 1000  # How often the policy network is transferred to the target network\n",
        "        self.batch_size = 32  # The amount of experiences used to train the network at one time\n",
        "        self.discount_factor = 0.99\n",
        "        self.learning_rate = 0.0001\n",
        "\n",
        "        # Epsilon parameters\n",
        "        self.epsilon_start = 1\n",
        "        self.epsilon_final = 0.01\n",
        "        self.epsilon_decay = 100000  # Time steps to go from start to final\n",
        "        self.epsilon_by_frame = lambda time_step: -(time_step - self.epsilon_decay) / self.epsilon_decay\n",
        "\n",
        "        self.experience = ReplayMemory(self.memory_size)\n",
        "\n",
        "        self.Policy_Net = DQN(environment, device).to(device)\n",
        "        #self.Policy_Net.to(self.Policy_Net.device)\n",
        "\n",
        "        self.Target_Net = DQN(environment, device).to(device)\n",
        "        #self.Target_Net.to(self.Policy_Net.device)\n",
        "        self.Target_Net.load_state_dict(self.Policy_Net.state_dict())\n",
        "        self.Target_Net.eval()\n",
        "        self.optimiser = optim.Adam(params=self.Policy_Net.parameters(), lr=self.learning_rate)\n",
        "        self.display_progress = display_progress\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"This is the function used to train the network\"\"\"\n",
        "\n",
        "        # Initialising Parameters used within the code:\n",
        "        num_ep = 0\n",
        "        frame_idx = 0\n",
        "        training = True\n",
        "        training_rewards = []\n",
        "        rewards = 0\n",
        "        work_path = \"models\"\n",
        "\n",
        "        # Begin with getting the initial state by resetting the environment (reset the environment like you have already\n",
        "        # seen and assign it to a state variable)\n",
        "        state = self.environment.reset()\n",
        "        # Training loop\n",
        "        if training:\n",
        "            self.Policy_Net.train()\n",
        "        while True:\n",
        "        # Determine the epsilon value based on the frame index with the anonymous function created in the constructor\n",
        "            gen_epsilon = self.epsilon_by_frame(frame_idx)\n",
        "        # Check if the generated epsilon value is less than epsilon file (i.e., the minimum epsilon can be) and\n",
        "        # if it is, set it to epsilon final\n",
        "            if gen_epsilon < self.epsilon_final:\n",
        "                gen_epsilon = self.epsilon_final\n",
        "        # Get the action from the policy network\n",
        "            action = self.Policy_Net.get_action(state, gen_epsilon)\n",
        "        # Pass the action into the environment and retrieve the next_state, reward and done flag from it\n",
        "            next_state, reward, done, info = self.environment.step(action)\n",
        "        # Save all of the information from this experience into the experience data set\n",
        "            self.experience.append(state, action, reward, done, next_state)\n",
        "        # Add the reward from this experience to the cumulative rewards count 'rewards'\n",
        "            rewards += reward\n",
        "        # Set the state variable to be the next_state for the next time step\n",
        "            state = next_state\n",
        "        # Increase the frame_idx count by 1\n",
        "            frame_idx += 1\n",
        "        # Check to see if there are enough experiences in the experience data set to train the model with (burn in)\n",
        "            if len(self.experience) >= self.batch_size:\n",
        "        # If there are, we want to then train our network (using the policy network's TD loss method we have\n",
        "        # already implemented)\n",
        "                self.Policy_Net.calc_TD_Loss(self.batch_size, model=self.Policy_Net, target_model=self.Target_Net, optimiser=self.optimiser, experience=self.experience, discount_factor=self.discount_factor)\n",
        "        # Check to see if the most recent time step finished the episode\n",
        "            if done:\n",
        "        # If so, the episode is now over\n",
        "\n",
        "        # Increase the num_ep counter by 1\n",
        "                num_ep += 1\n",
        "        # Print any useful information here (this part is up to you and for your own benefit when\n",
        "        # observing the training process)\n",
        "                print(f'Current number of episode {num_ep}/{self.max_episodes}')\n",
        "        # Store the episode's reward amount in the training_rewards array for graphing purposes\n",
        "                training_rewards.append(rewards)\n",
        "        # Reset the environment for the next episode and store the state\n",
        "                state = self.environment.reset()\n",
        "        # Reset the rewards count to 0 for the next episode\n",
        "                rewards = 0\n",
        "        # Check to see if enough episodes have passed to save the weights (save_freq)\n",
        "                if num_ep % self.save_freq == 0:\n",
        "        # Save the weights with the save_weights() function from utils.py\n",
        "                    save_weights(model=self.Policy_Net, path=self.fp, file_name='Weights_Policy.pt')\n",
        "                    save_weights(model=self.Target_Net, path=self.fp, file_name='Weights_Target.pt')\n",
        "        # Print anymore useful information here (again up to you)\n",
        "\n",
        "        # If you would like to see the current progress in the training (i.e., if display_progress is true)\n",
        "                    if self.display_progress:\n",
        "        # Then plot the results (again, in utils.py) - do note that a graph halts the interpreter: you\n",
        "        # have to close the graph before the training can continue\n",
        "                        plot_save_Results(training_rewards, self.fp)\n",
        "\n",
        "        # Check to see if enough frames have passed for us to transfer the policy weights to the target network\n",
        "            if frame_idx % self.target_update_freq == 0 and frame_idx != 0:\n",
        "        # If so, retrieve the state_dict() of the policy net and use load_state_dict() on the target net\n",
        "                self.Target_Net.load_state_dict(self.Policy_Net.state_dict())\n",
        "                self.Target_Net.eval()\n",
        "        # Check to see if enough episodes have passed to consider training complete (max_episodes)\n",
        "            if num_ep >= self.max_episodes:\n",
        "        # Save the weights for te final time\n",
        "                save_weights(model=self.Policy_Net, path=self.fp, file_name='Weights_Policy.pt')\n",
        "                save_weights(model=self.Target_Net, path=self.fp, file_name='Weights_Target.pt')\n",
        "        # Plot the results of training\n",
        "                plot_save_Results(training_rewards, self.fp)\n",
        "        # Break such that training does in fact stop\n",
        "                break\n",
        "\n",
        "    def learn(self):\n",
        "        \"\"\"This is the method that we will call from outside this class - think of it as a wrapper for the training\"\"\"\n",
        "\n",
        "        # Use time.time() to record the starting time of training\n",
        "        start_time = time.time()\n",
        "        # Start the training!\n",
        "        self.train()\n",
        "        # Record the end time of training\n",
        "        end_time = time.time()\n",
        "        # Find out how much time is spent training and display\n",
        "        diff_time = end_time - start_time\n",
        "        print(f'Time taken to complete training: {diff_time}')\n",
        "\n",
        "    def evaluate(self, game_speed=0.1):\n",
        "        \"\"\"This is the method used to evaluate the performance of the agent\"\"\"\n",
        "\n",
        "        Policy_weights_path = os.path.join(self.fp, 'Weights_Policy.pt')  # Getting the path to where the weights are saved\n",
        "\n",
        "        # Load the saved weights into our model and set it to evaluation mode\n",
        "        self.Policy_Net.load_state_dict(torch.load(Policy_weights_path))\n",
        "        self.Policy_Net.eval()\n",
        "\n",
        "        # Include Video Wrapper for video output\n",
        "        environment = wrap_env_Video(self.environment)\n",
        "        # Reset the environment and store the state\n",
        "        state = environment.reset()\n",
        "        # set epsilon to 0 (we don't want any randomness)\n",
        "        gen_epsilon = 0\n",
        "        # Starting the eval loop\n",
        "        while True:\n",
        "        # Render the environment\n",
        "            environment.render()\n",
        "        # Use time.sleep(), passing in game_speed. This is done to slow down the game as it is very fast be default\n",
        "        # to increase the speed of training\n",
        "            time.sleep(game_speed)\n",
        "        # Evaluate the action from the model\n",
        "            action = self.Policy_Net.get_action(state, gen_epsilon)\n",
        "        # Pass this action into the environment and store the resulting information\n",
        "            next_state, reward, done, info = environment.step(action)\n",
        "        # Update the state with next_state for the next time step\n",
        "            state = next_state\n",
        "        # Check if done to break\n",
        "            if done:\n",
        "        # Closing the environment and displaying the video\n",
        "                environment.close()\n",
        "                break\n",
        "        video_display()\n",
        "\n",
        "    def plot_view(self):\n",
        "        \"\"\"This method plots the view seen by the agent\"\"\"\n",
        "        self.environment.reset()\n",
        "\n",
        "        # Just a normal agent without any learning\n",
        "        action = self.environment.action_space.sample()\n",
        "        observation, reward, done, info = self.environment.step(action)\n",
        "\n",
        "        plt.figure(figsize=(20, 5))\n",
        "        for i in range(4):\n",
        "            plt.subplot(1, 4, i + 1)\n",
        "            plt.imshow(observation[i], cmap=plt.get_cmap('gray'))\n"
      ],
      "metadata": {
        "id": "64OfPAx7EHCo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "class DuelingDQN(nn.Module):\n",
        "\n",
        "    def __init__(self, env, device):\n",
        "        super(DuelingDQN, self).__init__()\n",
        "\n",
        "        # Parameters that will be used later\n",
        "        self.input_dim = env.observation_space.shape\n",
        "        self.num_actions = env.action_space.n\n",
        "        self.device = device\n",
        "\n",
        "        '''\n",
        "        First begin by creating the CNN\n",
        "        I suggest building the CNN using the nn.Sequential() method.\n",
        "        If you're not sure what to do for the CNN, I would suggest taking a look over the following\n",
        "        paper which details working with the OpenAi Atari Gym.\n",
        "        https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf\n",
        "        '''\n",
        "        self.CNN = nn.Sequential(\n",
        "            nn.Conv2d(in_channels= self.input_dim[0], out_channels= 32, kernel_size= (8,8), stride= 4),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels= 32, out_channels= 64, kernel_size= (4,4), stride= 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels= 64, out_channels= 64, kernel_size= (3,3), stride= 1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Determine the fully connected layer's input size using the supplied CNN_output_dim() method\n",
        "        self.fcl_input_size = self.CNN_output_dim(self.input_dim)\n",
        "\n",
        "        '''\n",
        "        Now build the fully connected layer.\n",
        "        Again, I suggest building it with the nn.Sequential() method.\n",
        "        If you found the model architecture content in the paper listed above, then the following\n",
        "        architecture will be very easy.\n",
        "        '''\n",
        "        self.fcl_advantage = nn.Sequential(\n",
        "            nn.Linear(self.fcl_input_size, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, self.num_actions)\n",
        "        )\n",
        "\n",
        "        self.fcl_state_value = nn.Sequential(\n",
        "            nn.Linear(self.fcl_input_size, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 1)\n",
        "        )\n",
        "\n",
        "    def CNN_output_dim(self, input_dim):\n",
        "        '''\n",
        "        This method is used to determine the output dimensions of the CNN (which is used\n",
        "        to set the input dimension of the fully connected layer).\n",
        "        '''\n",
        "        return self.CNN(torch.zeros(1, *input_dim)).flatten().shape[0]\n",
        "\n",
        "    def forward(self, X):\n",
        "        '''\n",
        "        This method is used to do a forward pass into the model, taking in the state and returning the Q value\n",
        "            1. Pass the state into the CNN\n",
        "            2. Flatten the output of the CNN using the flatten method with start_dim=1\n",
        "            3. Pass this flattened tensor into the fully connected layer\n",
        "            4. Return the output of the fully connected layer, i.e., the Q value\n",
        "        '''\n",
        "        X = self.CNN(X)\n",
        "        X = X.flatten(start_dim=1)\n",
        "        state_val_stream = self.fcl_state_value(X)\n",
        "        adv_stream = self.fcl_advantage(X)\n",
        "        Q = state_val_stream + adv_stream - torch.mean(adv_stream)\n",
        "        return Q\n",
        "\n",
        "    def get_action(self, state, epsilon):\n",
        "        '''\n",
        "        This methos is used to get the action that should be taken where the action will either be random (explore)\n",
        "        or calculated by the model (exploit).\n",
        "        '''\n",
        "        # Use random.random() here to generate a random float and if it's greater than epsilon, then exploit else explore\n",
        "        if random.uniform(0, 1) > epsilon:\n",
        "        # If exploiting, use the supplied epsilon_Greed_Strat() method to generate the action\n",
        "            new_action = self.epsilon_Greed_Strat(state)\n",
        "        # If exploring, use the random.randrange() method, passing in self.num_actions to generate the action\n",
        "        else:\n",
        "            new_action = random.randrange(self.num_actions)\n",
        "        # Then, return the action\n",
        "        return new_action\n",
        "\n",
        "    def epsilon_Greed_Strat(self, state):\n",
        "        '''\n",
        "        This method is used to determine the best action to take from a given state based on the previously-calculated\n",
        "        Q values.\n",
        "        '''\n",
        "        qval = self.get_qvals(state)\n",
        "        return qval.max(1)[1].data[0]\n",
        "\n",
        "    def get_qvals(self, state):\n",
        "        '''\n",
        "        This method is used to calculate the Q value of a given state, by passing the state into the model\n",
        "        '''\n",
        "        with torch.no_grad():\n",
        "            # Converting state data into tensor\n",
        "            state_t = torch.FloatTensor(np.float32(state)).unsqueeze(0).to(device=self.device)\n",
        "            return self.forward(state_t)\n",
        "\n",
        "    def calc_TD_Loss(self, batch_size, model, target_model, optimiser, experience, discount_factor, usePER=False, beta=0.0, offset=0.0):\n",
        "        \"\"\"The function will first take input from the exprience and then\n",
        "        calculate the optimum value from it. Later on, the Q-values are calculated\n",
        "        to give an idea of if it's converging to the optimal policy.\"\"\"\n",
        "\n",
        "        # Extract a sample of batches from our replay buffer (ReplayMemory class)\n",
        "        if usePER:\n",
        "            states, actions, rewards, dones, next_states, weights, batch_idxes = experience.sample(batch_size, beta)\n",
        "            weights_t = torch.FloatTensor(np.float32(weights)).to(self.device)\n",
        "        else:\n",
        "            states, actions, rewards, dones, next_states = experience.sample_batch(batch_size)\n",
        "\n",
        "        # The following code is typical of any loss-calculation and training method.\n",
        "\n",
        "        # Convert all of our tuples to tensors:\n",
        "        states = torch.FloatTensor(np.float32(states)).to(self.device)\n",
        "        next_states = torch.FloatTensor(np.float32(next_states)).to(self.device)\n",
        "        rewards_t = torch.FloatTensor(rewards).to(self.device)\n",
        "        actions_t = torch.LongTensor(np.int64(actions)).to(self.device)\n",
        "        done_t = torch.FloatTensor(dones).to(self.device)\n",
        "\n",
        "        # Calculate the value-action function as well as the value-action function\n",
        "        # for the next state\n",
        "        qvals = model(states)\n",
        "        qvals = qvals.gather(1, actions_t.unsqueeze(1)).squeeze(1)\n",
        "\n",
        "\n",
        "        next_qvals = model(next_states)\n",
        "        next_qval_state = target_model(next_states)\n",
        "        next_qval = next_qval_state.gather(1, torch.max(next_qvals, 1)[1].unsqueeze(1)).squeeze(1)\n",
        "\n",
        "        # Calculate towards the optimum value-action function\n",
        "        expected_qvals = rewards_t + discount_factor * next_qval * (1 - done_t)\n",
        "        # Calculate loss with Mean-Square Loss Function:\n",
        "\n",
        "\n",
        "        if usePER:\n",
        "            temporal_difference = torch.Tensor.cpu(expected_qvals - qvals)\n",
        "\n",
        "            new_priorities = offset + np.abs(temporal_difference.detach().numpy())\n",
        "            experience.update_priorities(batch_idxes, new_priorities)\n",
        "            #print(type(qvals), type(weights), type(expected_qvals))\n",
        "            \"\"\"with torch.no_grad():\n",
        "                weight = sum(np.multiply(weights, loss.data.cpu().numpy()))\n",
        "                loss *= weight\"\"\"\n",
        "            loss = F.mse_loss(qvals * weights_t, weights_t * expected_qvals.detach().to(device=model.device))\n",
        "        else:\n",
        "            loss = F.mse_loss(qvals, expected_qvals.detach().to(device=model.device))\n",
        "\n",
        "\n",
        "        # Backpropagate and update the model:\n",
        "        optimiser.zero_grad()\n",
        "        loss.backward()\n",
        "        optimiser.step()\n"
      ],
      "metadata": {
        "id": "lV54UNn9Z1Vx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch.optim as optim\n",
        "import time\n",
        "\n",
        "\n",
        "class DuelingAgent:\n",
        "    def __init__(self, environment, device, display_progress=True, usePER=False, useNoise=False, fp=\"C:\\\\Users\\\\Chong Yen Juin\\\\Desktop\\\\IntroRL\\\\save_model\"):\n",
        "        self.environment = environment\n",
        "        self.fp = fp  # The file path we will be saving our weights to\n",
        "        self.usePER = usePER\n",
        "        self.useNoise = useNoise\n",
        "        self.start_time = time.time()\n",
        "\n",
        "        # Hyperparameters - I strongly suggest you play around ith these to find better ones!\n",
        "        self.memory_size = 40000  # Maximum amount of experiences that can be stored\n",
        "        self.burn_in = 10000  # How many time steps should pass until the network learns (via TD Loss)\n",
        "        self.save_freq = 10  # How often the model gets saved to a .pt file, and the performance is graphed\n",
        "        self.max_episodes = 350  # The total number of episodes the agent will play in training\n",
        "        self.target_update_freq = 1000  # How often the policy network is transferred to the target network\n",
        "        self.batch_size = 32  # The amount of experiences used to train the network at one time\n",
        "        self.discount_factor = 0.99\n",
        "        self.learning_rate = 0.0001\n",
        "\n",
        "        # Epsilon parameters\n",
        "        self.epsilon_start = 1\n",
        "        self.epsilon_final = 0.01\n",
        "        self.epsilon_decay = 100000  # Time steps to go from start to final\n",
        "        self.epsilon_by_frame = lambda time_step: -(time_step - self.epsilon_decay) / self.epsilon_decay\n",
        "\n",
        "        self.experience = ReplayMemory(self.memory_size)\n",
        "\n",
        "        # Prioritized Replay Memory paramters\n",
        "        self.offset = 1e-6\n",
        "        self.alpha = 0.6\n",
        "        self.beta_initial = 0.4\n",
        "        self.beta_final = 1.0\n",
        "        self.beta_decay = 100000  # Time steps to go from start to final\n",
        "        self.beta_by_frame = lambda time_step: self.beta_initial + time_step *(self.beta_final - self.beta_initial) / self.beta_decay\n",
        "        self.P_experience = PrioritizedReplayBuffer(self.memory_size, self.alpha)\n",
        "\n",
        "\n",
        "        # DDQN\n",
        "        self.Policy_Net = DuelingDQN(environment, device).to(device)\n",
        "        #self.Policy_Net.to(self.Policy_Net.device)\n",
        "\n",
        "        self.Target_Net = DuelingDQN(environment, device).to(device)\n",
        "        #self.Target_Net.to(self.Policy_Net.device)\n",
        "        self.Target_Net.load_state_dict(self.Policy_Net.state_dict())\n",
        "        self.Target_Net.eval()\n",
        "        self.optimiser = optim.Adam(params=self.Policy_Net.parameters(), lr=self.learning_rate)\n",
        "        self.display_progress = display_progress\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"This is the function used to train the network\"\"\"\n",
        "\n",
        "        # Initialising Parameters used within the code:\n",
        "        num_ep = 0\n",
        "        frame_idx = 0\n",
        "        training = True\n",
        "        training_rewards = []\n",
        "        rewards = 0\n",
        "        work_path = \"models\"\n",
        "\n",
        "        # Begin with getting the initial state by resetting the environment (reset the environment like you have already\n",
        "        # seen and assign it to a state variable)\n",
        "        state = self.environment.reset()\n",
        "        # Training loop\n",
        "        if training:\n",
        "            self.Policy_Net.train()\n",
        "        while True:\n",
        "        # Determine the epsilon value based on the frame index with the anonymous function created in the constructor\n",
        "            gen_epsilon = self.epsilon_by_frame(frame_idx)\n",
        "        # Check if the generated epsilon value is less than epsilon file (i.e., the minimum epsilon can be) and\n",
        "        # if it is, set it to epsilon final\n",
        "            if gen_epsilon < self.epsilon_final:\n",
        "                gen_epsilon = self.epsilon_final\n",
        "        # Get the action from the policy network\n",
        "            action = self.Policy_Net.get_action(state, gen_epsilon)\n",
        "            if torch.is_tensor(action):\n",
        "                action = action.item()\n",
        "        # Pass the action into the environment and retrieve the next_state, reward and done flag from it\n",
        "            next_state, reward, done, info = self.environment.step(action)\n",
        "        # Save all of the information from this experience into the experience data set\n",
        "            if self.usePER:\n",
        "                self.P_experience.push(state, action, reward, done, next_state)\n",
        "            else:\n",
        "                self.experience.append(state, action, reward, done, next_state)\n",
        "        # Add the reward from this experience to the cumulative rewards count 'rewards'\n",
        "            rewards += reward\n",
        "        # Set the state variable to be the next_state for the next time step\n",
        "            state = next_state\n",
        "        # Increase the frame_idx count by 1\n",
        "            frame_idx += 1\n",
        "        # Check to see if there are enough experiences in the experience data set to train the model with (burn in)\n",
        "            if ((len(self.experience) >= self.burn_in) or (len(self.P_experience) >= self.burn_in)):\n",
        "        # If there are, we want to then train our network (using the policy network's TD loss method we have\n",
        "        # already implemented)\n",
        "                if self.usePER: #three more param, beta and usePER\n",
        "                    gen_beta = self.beta_by_frame(frame_idx)\n",
        "                    if gen_beta > self.beta_final:\n",
        "                        gen_beta = self.beta_final\n",
        "                    self.Policy_Net.calc_TD_Loss(self.batch_size, model=self.Policy_Net, target_model=self.Target_Net, optimiser=self.optimiser, experience=self.P_experience, discount_factor=self.discount_factor, usePER=self.usePER, beta=gen_beta, offset=self.offset)\n",
        "                else:\n",
        "                    self.Policy_Net.calc_TD_Loss(self.batch_size, model=self.Policy_Net, target_model=self.Target_Net, optimiser=self.optimiser, experience=self.experience, discount_factor=self.discount_factor)\n",
        "        # Check to see if the most recent time step finished the episode\n",
        "            if done:\n",
        "        # If so, the episode is now over\n",
        "\n",
        "        # Increase the num_ep counter by 1\n",
        "                num_ep += 1\n",
        "        # Print any useful information here (this part is up to you and for your own benefit when\n",
        "        # observing the training process)\n",
        "                print(f'Current number of episode {num_ep}/{self.max_episodes} with average time {(time.time() - self.start_time)/num_ep} per episode, r:{np.mean(np.array(training_rewards))}')\n",
        "        # Store the episode's reward amount in the training_rewards array for graphing purposes\n",
        "                training_rewards.append(rewards)\n",
        "        # Reset the environment for the next episode and store the state\n",
        "                state = self.environment.reset()\n",
        "        # Reset the rewards count to 0 for the next episode\n",
        "                rewards = 0\n",
        "        # Check to see if enough episodes have passed to save the weights (save_freq)\n",
        "                if num_ep % self.save_freq == 0:\n",
        "        # Save the weights with the save_weights() function from utils.py\n",
        "                    save_weights(model=self.Policy_Net, path=self.fp, file_name='DWeights_Policy.pt')\n",
        "                    save_weights(model=self.Target_Net, path=self.fp, file_name='DWeights_Target.pt')\n",
        "        # Print anymore useful information here (again up to you)\n",
        "\n",
        "        # If you would like to see the current progress in the training (i.e., if display_progress is true)\n",
        "                    if self.display_progress:\n",
        "        # Then plot the results (again, in utils.py) - do note that a graph halts the interpreter: you\n",
        "        # have to close the graph before the training can continue\n",
        "                        plot_save_Results(training_rewards, self.fp)\n",
        "\n",
        "        # Check to see if enough frames have passed for us to transfer the policy weights to the target network\n",
        "            if frame_idx % self.target_update_freq == 0 and frame_idx != 0:\n",
        "        # If so, retrieve the state_dict() of the policy net and use load_state_dict() on the target net\n",
        "                self.Target_Net.load_state_dict(self.Policy_Net.state_dict())\n",
        "                self.Target_Net.eval()\n",
        "        # Check to see if enough episodes have passed to consider training complete (max_episodes)\n",
        "            if num_ep >= self.max_episodes:\n",
        "        # Save the weights for te final time\n",
        "                save_weights(model=self.Policy_Net, path=self.fp, file_name='DWeights_Policy.pt')\n",
        "                save_weights(model=self.Target_Net, path=self.fp, file_name='DWeights_Target.pt')\n",
        "        # Plot the results of training\n",
        "                plot_save_Results(training_rewards, self.fp)\n",
        "        # Break such that training does in fact stop\n",
        "                break\n",
        "\n",
        "    def learn(self):\n",
        "        \"\"\"This is the method that we will call from outside this class - think of it as a wrapper for the training\"\"\"\n",
        "\n",
        "        # Use time.time() to record the starting time of training\n",
        "        start_time = time.time()\n",
        "        # Start the training!\n",
        "        self.train()\n",
        "        # Record the end time of training\n",
        "        end_time = time.time()\n",
        "        # Find out how much time is spent training and display\n",
        "        diff_time = end_time - start_time\n",
        "        print(f'Time taken to complete training: {diff_time}')\n",
        "\n",
        "    def evaluate(self, game_speed=0.1):\n",
        "        \"\"\"This is the method used to evaluate the performance of the agent\"\"\"\n",
        "\n",
        "        Policy_weights_path = os.path.join(self.fp, 'Weights_Policy.pt')  # Getting the path to where the weights are saved\n",
        "\n",
        "        # Load the saved weights into our model and set it to evaluation mode\n",
        "        self.Policy_Net.load_state_dict(torch.load(Policy_weights_path))\n",
        "        self.Policy_Net.eval()\n",
        "\n",
        "        # Include Video Wrapper for video output\n",
        "        environment = wrap_env_Video(self.environment)\n",
        "        # Reset the environment and store the state\n",
        "        state = environment.reset()\n",
        "        # set epsilon to 0 (we don't want any randomness)\n",
        "        gen_epsilon = 0\n",
        "        # Starting the eval loop\n",
        "        while True:\n",
        "        # Render the environment\n",
        "            environment.render()\n",
        "        # Use time.sleep(), passing in game_speed. This is done to slow down the game as it is very fast be default\n",
        "        # to increase the speed of training\n",
        "            time.sleep(game_speed)\n",
        "        # Evaluate the action from the model\n",
        "            action = self.Policy_Net.get_action(state, gen_epsilon)\n",
        "        # Pass this action into the environment and store the resulting information\n",
        "            next_state, reward, done, info = environment.step(action)\n",
        "        # Update the state with next_state for the next time step\n",
        "            state = next_state\n",
        "        # Check if done to break\n",
        "            if done:\n",
        "        # Closing the environment and displaying the video\n",
        "                environment.close()\n",
        "                break\n",
        "        video_display()\n",
        "\n",
        "    def plot_view(self):\n",
        "        \"\"\"This method plots the view seen by the agent\"\"\"\n",
        "        self.environment.reset()\n",
        "\n",
        "        # Just a normal agent without any learning\n",
        "        action = self.environment.action_space.sample()\n",
        "        observation, reward, done, info = self.environment.step(action)\n",
        "\n",
        "        plt.figure(figsize=(20, 5))\n",
        "        for i in range(4):\n",
        "            plt.subplot(1, 4, i + 1)\n",
        "            plt.imshow(observation[i], cmap=plt.get_cmap('gray'))\n"
      ],
      "metadata": {
        "id": "aR-t-kpaZ3fN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyvirtualdisplay import Display\n",
        "from common.wrappers import make_atari, wrap_deepmind, wrap_pytorch\n",
        "import gym\n",
        "from gym.wrappers import Monitor\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "from pyvirtualdisplay import Display\n",
        "from IPython import display as ipythondisplay\n",
        "import threading\n",
        "\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()\n",
        "\n",
        "is_ipython = 'inline' in matplotlib.get_backend()\n",
        "if is_ipython: from IPython import display\n",
        "\n",
        "t_lst = []\n",
        "\n",
        "# Loading the device, setting it to run on a GPU if available and on the CPU otherwise\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "# Initialising the environment\n",
        "environment = make_atari(\"PongNoFrameskip-v4\")\n",
        "# Adding required wrappers\n",
        "environment = wrap_pytorch(wrap_deepmind(environment, frame_stack=True))\n",
        "\n",
        "# Initialising Agent\n",
        "\n",
        "pongent = DuelingAgent(environment, device, display_progress=True, usePER=True, useNoise=False, fp=\"/content/gdrive/My Drive/RL_Team\")\n",
        "print(\"Start!\")\n",
        "pongent.learn()\n",
        "#pongent.evaluate()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 999
        },
        "id": "XBGfmT_iENl-",
        "outputId": "af45dec9-3238-42bc-e104-ab06fc41b128"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current number of episode 101/350 with average time 10.986475864259324 per episode, r:-20.34\n",
            "Current number of episode 102/350 with average time 10.995795004508075 per episode, r:-20.336633663366335\n",
            "Current number of episode 103/350 with average time 10.996966324963616 per episode, r:-20.34313725490196\n",
            "Current number of episode 104/350 with average time 10.997896570425768 per episode, r:-20.349514563106798\n",
            "Current number of episode 105/350 with average time 10.999122285842896 per episode, r:-20.35576923076923\n",
            "Current number of episode 106/350 with average time 11.000425131815785 per episode, r:-20.36190476190476\n",
            "Current number of episode 107/350 with average time 11.001103425694403 per episode, r:-20.367924528301888\n",
            "Current number of episode 108/350 with average time 11.001900383719692 per episode, r:-20.373831775700936\n",
            "Current number of episode 109/350 with average time 11.003291565343874 per episode, r:-20.37962962962963\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-8e4b17261f1f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mpongent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDuelingAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menvironment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisplay_progress\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musePER\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0museNoise\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/content/gdrive/My Drive/RL_Team\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Start!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mpongent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;31m#pongent.evaluate()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-d56f084c40b6>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0;31m# Start the training!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0;31m# Record the end time of training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-d56f084c40b6>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    100\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mgen_beta\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeta_final\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m                         \u001b[0mgen_beta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeta_final\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPolicy_Net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalc_TD_Loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPolicy_Net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTarget_Net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimiser\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimiser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mP_experience\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscount_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscount_factor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musePER\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musePER\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgen_beta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPolicy_Net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalc_TD_Loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPolicy_Net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTarget_Net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimiser\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimiser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperience\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscount_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscount_factor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-5b9321b97bd7>\u001b[0m in \u001b[0;36mcalc_TD_Loss\u001b[0;34m(self, batch_size, model, target_model, optimiser, experience, discount_factor, usePER, beta, offset)\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0moptimiser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m         \u001b[0moptimiser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    142\u001b[0m                    \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m                    \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight_decay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m                    eps=group['eps'])\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/_functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sqs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}