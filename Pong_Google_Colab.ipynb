{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1vQQ2Kf1gQB"
      },
      "source": [
        "### ONLY RUN THIS CELL IF THESE DEPENDENCIES ARE NOT YET INSTALLED ON YOUR DEVICE\n",
        "\n",
        "If you get some error saying XXX can't be found, then just run this\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l42lCbE61fOu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f001e57-ae0d-46f9-e915-0090ae75ac4d"
      },
      "source": [
        "# Rendering Dependencies\n",
        "!pip install gym==0.19.0 pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "\n",
        "# ATARI Dependencies\n",
        "!apt-get update > /dev/null 2>&1\n",
        "!apt-get install cmake > /dev/null 2>&1\n",
        "!pip install --upgrade setuptools 2>&1\n",
        "!pip install ez_setup > /dev/null 2>&1\n",
        "!pip install gym[atari] > /dev/null 2>&1\n",
        "#!pip install --upgrade gym 2>&1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (60.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCnY9o8e1pxY"
      },
      "source": [
        "## 1) Importing everything..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gInfLPxHt-Vr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49a40afa-3ca2-43eb-cf2a-ae669f9c2e1d"
      },
      "source": [
        "''' First we are going to import all the necessary libraries and directories'''\n",
        "\n",
        "# Import the main bois: Gym and other standard libraries\n",
        "import gym\n",
        "from gym import spaces\n",
        "import numpy as np\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# Import all necessary torch libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as T\n",
        "import torch.autograd as autograd\n",
        "\n",
        "# Rendering Dependencies\n",
        "from collections import namedtuple\n",
        "from PIL import Image\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers import Monitor\n",
        "gymlogger.set_level(40) #error only\n",
        "import io\n",
        "import glob\n",
        "import base64\n",
        "from IPython.display import HTML # The main dude who's gonna create the visualization\n",
        "\n",
        "from IPython import display as ipythondisplay\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# Processing Dependencies\n",
        "from collections import deque\n",
        "import time\n",
        "\n",
        "# Souvenir Shop\n",
        "from google.colab import drive\n",
        "import sys\n",
        "\n",
        "#Mount your Google drive to the VM\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayMemory(object):\n",
        "\n",
        "    def __init__(self, memory_size):\n",
        "        '''Setup the class variables required for the Replay Memory:\n",
        "            - The memory size\n",
        "            - The data structure where the experience tuples will be stored\n",
        "              (I suggest a deque)\n",
        "        '''\n",
        "        self.memory_size = memory_size\n",
        "        self.replay_memory_deque = deque(maxlen = self.memory_size)\n",
        "\n",
        "    def sample_batch(self, batch_size=32):\n",
        "        '''\n",
        "        Take a random set of experiences from the Replay Memory deque and return the values from each experience\n",
        "        in their own respective arrays ->\n",
        "        return state, action, reward, done, next_state\n",
        "            - state: array of the state values from the random set of experiences\n",
        "            - action: array of the action values from the random set of experiences\n",
        "            - reward: array of the reward values from the random set of experiences\n",
        "            - done: array of the done values from the random set of experiences\n",
        "            - next_state: array of the next_state values from the random set of experiences\n",
        "        '''\n",
        "        if len(self.replay_memory_deque) < batch_size:\n",
        "            return array(), array(), array(), array(), array()\n",
        "        state, action, reward, done, next_state = zip(*random.sample(self.replay_memory_deque, k = batch_size))\n",
        "        return array(state), array(action), array(reward), array(done), array(next_state)\n",
        "\n",
        "    # Push the data into the memory space\n",
        "    def append(self, state, action, reward, done, next_state):\n",
        "        '''\n",
        "        Append the data pased into the method from an experience into the deque\n",
        "        '''\n",
        "\n",
        "        self.replay_memory_deque.append((state, action, reward, done, next_state))\n",
        "\n",
        "    def __len__(self):\n",
        "        '''\n",
        "        Return the length of the deque\n",
        "        '''\n",
        "        return len(self.replay_memory_deque)"
      ],
      "metadata": {
        "id": "2Mt8SbuXD7kV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DQN(nn.Module):\n",
        "\n",
        "    def __init__(self, env, device):\n",
        "        super(DQN, self).__init__()\n",
        "\n",
        "        # Parameters that will be used later\n",
        "        self.input_dim = env.observation_space.shape\n",
        "        self.num_actions = env.action_space.n\n",
        "        self.device = device\n",
        "\n",
        "        '''\n",
        "        First begin by creating the CNN\n",
        "        I suggest building the CNN using the nn.Sequential() method.\n",
        "        If you're not sure what to do for the CNN, I would suggest taking a look over the following\n",
        "        paper which details working with the OpenAi Atari Gym.\n",
        "        https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf\n",
        "        '''\n",
        "        self.CNN = nn.Sequential(\n",
        "            nn.Conv2d(in_channels= self.input_dim[2], out_channels= 32, kernel_size= 8, stride= 4),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels= 32, out_channels= 64, kernel_size= 4, stride= 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels= 64, out_channels= 64, kernel_size= 3, stride= 1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Determine the fully connected layer's input size using the supplied CNN_output_dim() method\n",
        "        self.fcl_input_size = self.CNN_output_dim(self.input_dim)\n",
        "\n",
        "        '''\n",
        "        Now build the fully connected layer.\n",
        "        Again, I suggest building it with the nn.Sequential() method.\n",
        "        If you found the model architecture content in the paper listed above, then the following\n",
        "        architecture will be very easy.\n",
        "        '''\n",
        "        self.fcl = nn.Sequential(\n",
        "            nn.Linear(self.fcl_input_size, 512),\n",
        "            nn.ReLu(),\n",
        "            nn.Linear(512, self.num_actions)\n",
        "        )\n",
        "\n",
        "    def CNN_output_dim(self, input_dim):\n",
        "        '''\n",
        "        This method is used to determine the output dimensions of the CNN (which is used\n",
        "        to set the input dimension of the fully connected layer).\n",
        "        '''\n",
        "        return self.CNN(torch.zeros(1, *input_dim)).flatten().shape[0]\n",
        "\n",
        "    def forward(self, X):\n",
        "        '''\n",
        "        This method is used to do a forward pass into the model, taking in the state and returning the Q value\n",
        "            1. Pass the state into the CNN\n",
        "            2. Flatten the output of the CNN using the flatten method with start_dim=1\n",
        "            3. Pass this flattened tensor into the fully connected layer\n",
        "            4. Return the output of the fully connected layer, i.e., the Q value\n",
        "        '''\n",
        "        X = self.CNN(X)\n",
        "        X = X.flatten(start_dim=1)\n",
        "        X = self.fcl(X)\n",
        "        return X\n",
        "\n",
        "    def get_action(self, state, epsilon):\n",
        "        '''\n",
        "        This methos is used to get the action that should be taken where the action will either be random (explore)\n",
        "        or calculated by the model (exploit).\n",
        "        '''\n",
        "        # Use random.random() here to generate a random float and if it's greater than epsilon, then exploit else explore\n",
        "        if random.uniform(0, 1) > epsilon:\n",
        "        # If exploiting, use the supplied epsilon_Greed_Strat() method to generate the action\n",
        "            new_action = self.epsilon_Greed_Strat(state)\n",
        "        # If exploring, use the random.randrange() method, passing in self.num_actions to generate the action\n",
        "        else:\n",
        "            new_action = random.randrange(self.num_actions)\n",
        "        # Then, return the action\n",
        "        return new_action\n",
        "\n",
        "    def epsilon_Greed_Strat(self, state):\n",
        "        '''\n",
        "        This method is used to determine the best action to take from a given state based on the previously-calculated\n",
        "        Q values.\n",
        "        '''\n",
        "        qval = self.get_qvals(state)\n",
        "        return qval.max(1)[1].data[0]\n",
        "\n",
        "    def get_qvals(self, state):\n",
        "        '''\n",
        "        This method is used to calculate the Q value of a given state, by passing the state into the model\n",
        "        '''\n",
        "        with torch.no_grad():\n",
        "            # Converting state data into tensor\n",
        "            state_t = torch.FloatTensor(np.float32(state)).unsqueeze(0).to(device=self.device)\n",
        "            return self.forward(state_t)\n",
        "\n",
        "    def calc_TD_Loss(self, batch_size, model, target_model, optimiser, experience, discount_factor):\n",
        "        \"\"\"The function will first take input from the exprience and then\n",
        "        calculate the optimum value from it. Later on, the Q-values are calculated\n",
        "        to give an idea of if it's converging to the optimal policy.\"\"\"\n",
        "\n",
        "        # Extract a sample of batches from our replay buffer (ReplayMemory class)\n",
        "        states, actions, rewards, dones, next_states = ReplayMemory.sample_batch(32)\n",
        "\n",
        "        # The following code is typical of any loss-calculation and training method.\n",
        "\n",
        "        # Convert all of our tuples to tensors:\n",
        "        states = torch.FloatTensor(np.float32(states)).to(self.device)\n",
        "        next_states = torch.FloatTensor(np.float32(next_states)).to(self.device)\n",
        "        rewards_t = torch.FloatTensor(rewards).to(self.device)\n",
        "        actions_t = torch.LongTensor(actions).to(self.device)\n",
        "        done_t = torch.FloatTensor(dones).to(self.device)\n",
        "\n",
        "        # Calculate the value-action function as well as the value-action function\n",
        "        # for the next state\n",
        "        qvals = model(states)\n",
        "        qvals = qvals.gather(1, actions_t.unsqueeze(1)).squeeze(1)\n",
        "\n",
        "        # (2,1)\n",
        "        indices = actions_t.unsqueeze(1)\n",
        "        qvals = qvals.gather(1, indices)\n",
        "        qvals = qvals.squeeze(1)\n",
        "        # (2,)\n",
        "\n",
        "        # lst = [1,2,3]\n",
        "        # a = lst[1]\n",
        "\n",
        "        # index_tensor = [0, 1, 1, 0]\n",
        "\n",
        "\n",
        "        next_qvals = model(next_states)\n",
        "        next_qval_state = target_model(next_states)\n",
        "        next_qval = next_qval_state.gather(1, torch.max(next_qvals, 1)[1].unsqueeze(1)).squeeze(1)\n",
        "\n",
        "        # Calculate towards the optimum value-action function\n",
        "        expected_qvals = rewards_t + discount_factor * next_qval * (1 - done_t)\n",
        "\n",
        "        # Calculate loss with Mean-Square Loss Function:\n",
        "        loss = F.mse_loss(qvals, expected_qvals.detach().to(device=model.device))\n",
        "\n",
        "        # Backpropagate and update the model:\n",
        "        optimiser.zero_grad()\n",
        "        loss.backward()\n",
        "        optimiser.step()\n"
      ],
      "metadata": {
        "id": "WRmj1I5lD9-g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def video_display():\n",
        "    mp4Video = glob.glob('video/*.mp4')\n",
        "    if len(mp4Video) > 0:\n",
        "        mp4 = mp4Video[0]\n",
        "        video = io.open(mp4, 'r+b').read()\n",
        "        encoded = base64.b64encode(video)\n",
        "        ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay\n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "    else:\n",
        "        print(\"Could not find video\")\n",
        "\n",
        "\n",
        "def wrap_env_Video(env):\n",
        "    env = Monitor(env, './video', force=True)\n",
        "    return env\n",
        "\n",
        "\n",
        "def plot_save_Results(training_rewards, path):\n",
        "  clear_output(True)\n",
        "  plt.figure(figsize=(12,8))\n",
        "  plt.plot(training_rewards, label='Rewards')\n",
        "  plt.xlabel('Episodes')\n",
        "  plt.ylabel('Rewards')\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def save_weights(model, path, file_name=None):\n",
        "  if file_name is None:\n",
        "    file_name = 'your_models_trained_weights.pt'\n",
        "  weights_path = os.path.join(path,file_name)\n",
        "  torch.save(model.state_dict(), weights_path)\n",
        "def video_display():\n",
        "    mp4Video = glob.glob('video/*.mp4')\n",
        "    if len(mp4Video) > 0:\n",
        "        mp4 = mp4Video[0]\n",
        "        video = io.open(mp4, 'r+b').read()\n",
        "        encoded = base64.b64encode(video)\n",
        "        ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay\n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "    else:\n",
        "        print(\"Could not find video\")\n",
        "\n",
        "\n",
        "def wrap_env_Video(env):\n",
        "    env = Monitor(env, './video', force=True)\n",
        "    return env\n",
        "\n",
        "\n",
        "def plot_save_Results(training_rewards, path):\n",
        "  clear_output(True)\n",
        "  plt.figure(figsize=(12,8))\n",
        "  plt.plot(training_rewards, label='Rewards')\n",
        "  plt.xlabel('Episodes')\n",
        "  plt.ylabel('Rewards')\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def save_weights(model, path, file_name=None):\n",
        "  if file_name is None:\n",
        "    file_name = 'your_models_trained_weights.pt'\n",
        "  weights_path = os.path.join(path,file_name)\n",
        "  torch.save(model.state_dict(), weights_path)\n"
      ],
      "metadata": {
        "id": "rmyMfI7wEDSX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent:\n",
        "    def __init__(self, environment, device, display_progress=True, fp=\"model.pt\"):\n",
        "        self.environment = environment\n",
        "        self.fp = fp  # The file path we will be saving our weights to\n",
        "\n",
        "        # Hyperparameters - I strongly suggest you play around ith these to find better ones!\n",
        "        self.memory_size = 20000  # Maximum amount of experiences that can be stored\n",
        "        self.burn_in = 10000  # How many time steps should pass until the network learns (via TD Loss)\n",
        "        self.save_freq = 50  # How often the model gets saved to a .pt file, and the performance is graphed\n",
        "        self.max_episodes = 350  # The total number of episodes the agent will play in training\n",
        "        self.target_update_freq = 1000  # How often the policy network is transferred to the target network\n",
        "        self.batch_size = 32  # The amount of experiences used to train the network at one time\n",
        "        self.discount_factor = 0.99\n",
        "        self.learning_rate = 0.0001\n",
        "\n",
        "        # Epsilon parameters\n",
        "        self.epsilon_start = 1\n",
        "        self.epsilon_final = 0.01\n",
        "        self.epsilon_decay = 100000  # Time steps to go from start to final\n",
        "        self.epsilon_by_frame = lambda time_step: -(time_step - self.epsilon_decay) / self.epsilon_decay\n",
        "\n",
        "        self.experience = ReplayMemory(self.memory_size)\n",
        "\n",
        "        self.Policy_Net = DQN(environment, device)\n",
        "        self.Policy_Net.to(self.Policy_Net.device)\n",
        "\n",
        "        self.Target_Net = DQN(environment, device)\n",
        "        self.Target_Net.to(self.Policy_Net.device)\n",
        "        self.Target_Net.load_state_dict(self.Policy_Net.state_dict())\n",
        "        self.Target_Net.eval()\n",
        "        self.optimiser = optim.Adam(params=self.Policy_Net.parameters(), lr=self.learning_rate)\n",
        "        self.display_progress = display_progress\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"This is the function used to train the network\"\"\"\n",
        "\n",
        "        # Initialising Parameters used within the code:\n",
        "        num_ep = 0\n",
        "        frame_idx = 0\n",
        "        training = True\n",
        "        training_rewards = []\n",
        "        rewards = 0\n",
        "        work_path = \"models\"\n",
        "\n",
        "        # Begin with getting the initial state by resetting the environment (reset the environment like you have already\n",
        "        # seen and assign it to a state variable)\n",
        "        self.environment.reset()\n",
        "        state = self.environment.observation_space()\n",
        "        # Training loop\n",
        "        if training:\n",
        "            self.Policy_Net.train()\n",
        "        while True:\n",
        "        # Determine the epsilon value based on the frame index with the anonymous function created in the constructor\n",
        "            gen_epsilon = self.epsilon_by_frame(frame_idx)\n",
        "        # Check if the generated epsilon value is less than epsilon file (i.e., the minimum epsilon can be) and\n",
        "        # if it is, set it to epsilon final\n",
        "            if gen_epsilon < self.epsilon_final:\n",
        "                gen_epsilon = self.epsilon_final\n",
        "        # Get the action from the policy network\n",
        "            action = self.Policy_Net.get_action(state, gen_epsilon)\n",
        "        # Pass the action into the environment and retrieve the next_state, reward and done flag from it\n",
        "            next_state, reward, done, info = self.environment.step(action)\n",
        "        # Save all of the information from this experience into the experience data set\n",
        "            self.experience.append(state, action, reward, done, next_state)\n",
        "        # Add the reward from this experience to the cumulative rewards count 'rewards'\n",
        "            rewards += reward\n",
        "        # Set the state variable to be the next_state for the next time step\n",
        "            state = next_state\n",
        "        # Increase the frame_idx count by 1\n",
        "            frame_idx += 1\n",
        "        # Check to see if there are enough experiences in the experience data set to train the model with (burn in)\n",
        "            if len(self.experience) >= self.batch_size:\n",
        "        # If there are, we want to then train our network (using the policy network's TD loss method we have\n",
        "        # already implemented)\n",
        "                self.Policy_Net.calc_TD_Loss(self.batch_size, model=self.Policy_Net, target_model=self.Target_Net, optmiser=self.optimiser, experience=self.experience, discount_factor=self.discount_factor)\n",
        "        # Check to see if the most recent time step finished the episode\n",
        "            if done:\n",
        "        # If so, the episode is now over\n",
        "\n",
        "        # Increase the num_ep counter by 1\n",
        "                num_ep += 1\n",
        "        # Print any useful information here (this part is up to you and for your own benefit when\n",
        "        # observing the training process)\n",
        "\n",
        "        # Store the episode's reward amount in the training_rewards array for graphing purposes\n",
        "                training_rewards.append(rewards)\n",
        "        # Reset the environment for the next episode and store the state\n",
        "                state = self.environment.reset()\n",
        "        # Reset the rewards count to 0 for the next episode\n",
        "                rewards = 0\n",
        "        # Check to see if enough episodes have passed to save the weights (save_freq)\n",
        "                if num_ep % self.save_freq == 0:\n",
        "        # Save the weights with the save_weights() function from utils.py\n",
        "                    save_weights(model=self.Policy_Net, path=self.fp, file_name='Weights_Policy.pt')\n",
        "                    save_weights(model=self.Target_Net, path=self.fp, file_name='Weights_Target.pt')\n",
        "        # Print anymore useful information here (again up to you)\n",
        "\n",
        "        # If you would like to see the current progress in the training (i.e., if display_progress is true)\n",
        "\n",
        "        # Then plot the results (again, in utils.py) - do note that a graph halts the interpreter: you\n",
        "        # have to close the graph before the training can continue\n",
        "                    # plot_save_Results(training_rewards, self.fp)\n",
        "\n",
        "        # Check to see if enough frames have passed for us to transfer the policy weights to the target network\n",
        "            if frame_idx % self.target_update_freq == 0 and frame_idx != 0:\n",
        "        # If so, retrieve the state_dict() of the policy net and use load_state_dict() on the target net\n",
        "                self.Target_Net.load_state_dict(self.Policy_Net.state_dict())\n",
        "                self.Target_Net.eval()\n",
        "        # Check to see if enough episodes have passed to consider training complete (max_episodes)\n",
        "            if num_ep >= self.max_episodes:\n",
        "        # Save the weights for te final time\n",
        "                save_weights(model=self.Policy_Net, path=self.fp, file_name='Weights_Policy.pt')\n",
        "                save_weights(model=self.Target_Net, path=self.fp, file_name='Weights_Target.pt')\n",
        "        # Plot the results of training\n",
        "                plot_save_Results(training_rewards, self.fp)\n",
        "        # Break such that training does in fact stop\n",
        "                break\n",
        "\n",
        "    def learn(self):\n",
        "        \"\"\"This is the method that we will call from outside this class - think of it as a wrapper for the training\"\"\"\n",
        "\n",
        "        # Use time.time() to record the starting time of training\n",
        "        start_time = time.time()\n",
        "        # Start the training!\n",
        "        self.train()\n",
        "        # Record the end time of training\n",
        "        end_time = time.time()\n",
        "        # Find out how much time is spent training and display\n",
        "        diff_time = end_time - start_time\n",
        "        print(f'Time taken to complete training: {diff_time}')\n",
        "\n",
        "    def evaluate(self, game_speed=0.1):\n",
        "        \"\"\"This is the method used to evaluate the performance of the agent\"\"\"\n",
        "\n",
        "        Policy_weights_path = os.path.join(self.fp, 'Weights_Policy.pt')  # Getting the path to where the weights are saved\n",
        "\n",
        "        # Load the saved weights into our model and set it to evaluation mode\n",
        "        self.Policy_Net.load_state_dict(torch.load(Policy_weights_path))\n",
        "        self.Policy_Net.eval()\n",
        "\n",
        "        # Include Video Wrapper for video output\n",
        "        environment = wrap_env_Video(self.environment)\n",
        "        # Reset the environment and store the state\n",
        "        state = environment.reset()\n",
        "        # set epsilon to 0 (we don't want any randomness)\n",
        "        gen_epsilon = 0\n",
        "        # Starting the eval loop\n",
        "        while True:\n",
        "        # Render the environment\n",
        "            environment.render()\n",
        "        # Use time.sleep(), passing in game_speed. This is done to slow down the game as it is very fast be default\n",
        "        # to increase the speed of training\n",
        "            time.sleep(game_speed)\n",
        "        # Evaluate the action from the model\n",
        "            action = self.Policy_Net.get_action(state, gen_epsilon)\n",
        "        # Pass this action into the environment and store the resulting information\n",
        "            next_state, reward, done, info = environment.step(action)\n",
        "        # Update the state with next_state for the next time step\n",
        "            state = next_state\n",
        "        # Check if done to break\n",
        "            if done:\n",
        "        # Closing the environment and displaying the video\n",
        "                environment.close()\n",
        "                break\n",
        "        video_display()\n",
        "\n",
        "    def plot_view(self):\n",
        "        \"\"\"This method plots the view seen by the agent\"\"\"\n",
        "        self.environment.reset()\n",
        "\n",
        "        # Just a normal agent without any learning\n",
        "        action = self.environment.action_space.sample()\n",
        "        observation, reward, done, info = self.environment.step(action)\n",
        "\n",
        "        plt.figure(figsize=(20, 5))\n",
        "        for i in range(4):\n",
        "            plt.subplot(1, 4, i + 1)\n",
        "            plt.imshow(observation[i], cmap=plt.get_cmap('gray'))\n"
      ],
      "metadata": {
        "id": "64OfPAx7EHCo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyvirtualdisplay import Display\n",
        "from common.wrappers import make_atari, wrap_deepmind, wrap_pytorch\n",
        "\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()\n",
        "\n",
        "is_ipython = 'inline' in matplotlib.get_backend()\n",
        "if is_ipython: from IPython import display\n",
        "\n",
        "\n",
        "# Loading the device, setting it to run on a GPU if available and on the CPU otherwise\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Initialising the environment\n",
        "environment = make_atari(\"PongNoFrameskip-v4\")\n",
        "# Adding required wrappers\n",
        "environment = wrap_pytorch(wrap_deepmind(environment, frame_stack=True))\n",
        "\n",
        "# Initialising Agent\n",
        "Agent = Agent(environment, device, fp=\"model-pong.pt\")\n",
        "Agent.learn()\n",
        "#Agent.evaluate()\n"
      ],
      "metadata": {
        "id": "XBGfmT_iENl-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}